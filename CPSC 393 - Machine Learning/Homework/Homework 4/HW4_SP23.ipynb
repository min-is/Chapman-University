{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fdSjLCfLP9B"
      },
      "source": [
        "# Homework 4 (Sequential Models)\n",
        "\n",
        "1. Choose a book or other text to train your model on (I suggest [Project Gutenberg](https://www.gutenberg.org/ebooks/) to find .txt files but you can find them elsewhere). Make sure your file is a `.txt` file. Clean your data (You may use [this file](https://colab.research.google.com/drive/1HCgKn5XQ7Q3ywxGszVWx2kddfT9UBASp?usp=sharing) we talked about in class as a baseline). Build a sequential model (LSTM, GRU, SimpleRNN, or Transformer) that generates new lines based on the training data (NO TRANSFER LEARNING).\n",
        "\n",
        "Print out or write 10 generated sequences from your model (Similar to Classwork 17 where we generated new Pride and Prejudice lines, but now with words instead of charachters. Feel free to use [this](https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/) as a reference). Assess in detail how good they are, what they're good at, what they struggle to do well. \n",
        "\n",
        "2. Make a new model with ONE substantial adjustment (e.g. use a custom embedding layer if you didn't already, use a pre-trained embedding layer if you didn't already, use a DEEP LSTM/GRU with multiple recurrent layers, use a pre-trained model to do transfer learning and fine-tune it...etc.). \n",
        "\n",
        "Print out or write 10 generated sequences from your model (Similar to Classwork 17 where we generated new Pride and Prejudice lines, but now with words instead of charachters. Feel free to use [this](https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/) as a reference). Assess in detail how good they are, what they're good at, what they struggle to do well.  Did the performance of your model change?\n",
        "\n",
        "3. Then create a **technical report** discussing your model building process, the results, and your reflection on it. The report should follow the format in the example including an Introduction, Analysis, Methods, Results, and Reflection section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "THE BOY WHO LIVED \n",
            "\n",
            "Mr. and Mrs. Dursley, of number four, Privet Drive, \n",
            "were proud to say that they were perfectly normal, \n",
            "thank you very much. They were the last people youâ€™d \n",
            "expect to be involved\n",
            "['the', 'boy', 'who', 'lived', 'mr', 'and', 'mrs', 'dursley', 'of', 'number', 'four', 'privet', 'drive', 'were', 'proud', 'to', 'say', 'that', 'they', 'were', 'perfectly', 'normal', 'thank', 'you', 'very', 'much', 'they', 'were', 'the', 'last', 'people', 'expect', 'to', 'be', 'involved', 'in', 'anything', 'strange', 'or', 'mysterious', 'because', 'they', 'just', 'hold', 'with', 'such', 'nonsense', 'mr', 'dursley', 'was', 'the', 'director', 'of', 'a', 'firm', 'called', 'grunnings', 'which', 'made', 'drills', 'he', 'was', 'a', 'big', 'beefy', 'man', 'with', 'hardly', 'any', 'neck', 'although', 'he', 'did', 'have', 'a', 'very', 'large', 'mustache', 'mrs', 'dursley', 'was', 'thin', 'and', 'blonde', 'and', 'had', 'nearly', 'twice', 'the', 'usual', 'amount', 'of', 'neck', 'which', 'came', 'in', 'very', 'useful', 'as', 'she', 'spent', 'so', 'much', 'of', 'her', 'time', 'craning', 'over', 'garden', 'fences', 'spying', 'on', 'the', 'neighbors', 'the', 'dursley', 's', 'had', 'a', 'small', 'son', 'called', 'dudley', 'and', 'in', 'their', 'opinion', 'there', 'was', 'no', 'finer', 'boy', 'anywhere', 'the', 'dursleys', 'had', 'everything', 'they', 'wanted', 'but', 'they', 'also', 'had', 'a', 'secret', 'and', 'their', 'greatest', 'fear', 'was', 'that', 'somebody', 'would', 'discover', 'it', 'they', 'think', 'they', 'could', 'bear', 'it', 'if', 'anyone', 'found', 'out', 'about', 'the', 'potters', 'mrs', 'potter', 'was', 'mrs', 'sister', 'but', 'they', 'page', 'harry', 'potter', 'and', 'the', 'philosophers', 'stone', 'jk', 'rowling', 'met', 'for', 'several', 'years', 'in', 'fact', 'mrs', 'dursley', 'pretended', 'she', 'have', 'a', 'sister', 'because', 'her', 'sister']\n",
            "Total Tokens: 73643\n",
            "Unique Tokens: 5561\n",
            "Total Sequences: 73542\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import string\n",
        "\n",
        "my_file = '/Users/isaacmin/Desktop/code/philosopherstone.txt'\n",
        "seq_len = 100\n",
        "\n",
        "def load_doc(filename):\n",
        "    file = open(filename, 'r')\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "def clean_doc(doc):\n",
        "    doc = doc.replace('--', ' ')\n",
        "    tokens = doc.split()\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    tokens = [w.translate(table) for w in tokens]\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "    return tokens\n",
        "\n",
        "def save_doc(lines, filename):\n",
        "    data = '\\n'.join(lines)\n",
        "    file = open(filename, 'w')\n",
        "    file.write(data)\n",
        "    file.close()\n",
        "\n",
        "my_file_seq ='/Users/isaacmin/Desktop/code/philosopherstone.txt'\n",
        "doc = load_doc(my_file)\n",
        "print(doc[:200])\n",
        "\n",
        "# clean document\n",
        "tokens = clean_doc(doc)\n",
        "print(tokens[:200])\n",
        "print('Total Tokens: %d' % len(tokens))\n",
        "print('Unique Tokens: %d' % len(set(tokens)))\n",
        "\n",
        "# organize into sequences of tokens\n",
        "length = seq_len + 1\n",
        "sequences = list()\n",
        "for i in range(length, len(tokens)):\n",
        " # select sequence of tokens\n",
        " seq = tokens[i-length:i]\n",
        " # convert into a line\n",
        " line = ' '.join(seq)\n",
        " # store\n",
        " sequences.append(line)\n",
        "print('Total Sequences: %d' % len(sequences))\n",
        " \n",
        "# save sequences to file\n",
        "out_filename = my_file[:-4] + '_sequences.txt'\n",
        "save_doc(sequences, out_filename)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load\n",
        "in_filename = '/Users/isaacmin/Desktop/code/philosopherstone_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')\n",
        "\n",
        "# create a tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)\n",
        "\n",
        "# vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# separate into input and output\n",
        "sequences = np.array(sequences)\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "y = to_categorical(y, num_classes = vocab_size)\n",
        "seq_length = X.shape[1]\n",
        "\n",
        "p_train = 0.8\n",
        "\n",
        "n_train = int(X.shape[0]//(1/p_train))\n",
        "X_train = X[0:n_train]\n",
        "y_train = y[0:n_train]\n",
        "X_test = X[n_train:]\n",
        "y_test = y[n_train:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(58833, 100)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(14709, 100)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(58833, 5562)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(14709, 5562)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-26 15:31:41.311849: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 50)           278100    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 100, 100)          60400     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 100)               80400     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 5562)              561762    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 990,762\n",
            "Trainable params: 990,762\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 50, input_length = seq_length))\n",
        "model.add(LSTM(100, return_sequences = True))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(100, activation = 'relu'))\n",
        "model.add(Dense(vocab_size, activation = 'softmax'))\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "575/575 [==============================] - 147s 251ms/step - loss: 5.6792 - accuracy: 0.1241\n",
            "Epoch 2/100\n",
            "575/575 [==============================] - 140s 244ms/step - loss: 5.4868 - accuracy: 0.1307\n",
            "Epoch 3/100\n",
            "575/575 [==============================] - 143s 249ms/step - loss: 5.3590 - accuracy: 0.1359\n",
            "Epoch 4/100\n",
            "575/575 [==============================] - 145s 252ms/step - loss: 5.2604 - accuracy: 0.1398\n",
            "Epoch 5/100\n",
            "575/575 [==============================] - 143s 249ms/step - loss: 5.1636 - accuracy: 0.1446\n",
            "Epoch 6/100\n",
            "575/575 [==============================] - 139s 242ms/step - loss: 5.0780 - accuracy: 0.1472\n",
            "Epoch 7/100\n",
            "575/575 [==============================] - 140s 243ms/step - loss: 5.0024 - accuracy: 0.1512\n",
            "Epoch 8/100\n",
            "575/575 [==============================] - 140s 244ms/step - loss: 4.9270 - accuracy: 0.1546\n",
            "Epoch 9/100\n",
            "575/575 [==============================] - 169s 294ms/step - loss: 4.8612 - accuracy: 0.1571\n",
            "Epoch 10/100\n",
            "575/575 [==============================] - 222s 387ms/step - loss: 4.8013 - accuracy: 0.1600\n",
            "Epoch 11/100\n",
            "575/575 [==============================] - 215s 375ms/step - loss: 4.7597 - accuracy: 0.1628\n",
            "Epoch 12/100\n",
            "575/575 [==============================] - 216s 376ms/step - loss: 4.6994 - accuracy: 0.1656\n",
            "Epoch 13/100\n",
            "575/575 [==============================] - 216s 375ms/step - loss: 4.6256 - accuracy: 0.1675\n",
            "Epoch 14/100\n",
            "575/575 [==============================] - 206s 358ms/step - loss: 4.5406 - accuracy: 0.1710\n",
            "Epoch 15/100\n",
            "575/575 [==============================] - 141s 245ms/step - loss: 4.4725 - accuracy: 0.1729\n",
            "Epoch 16/100\n",
            "575/575 [==============================] - 155s 270ms/step - loss: 4.4542 - accuracy: 0.1742\n",
            "Epoch 17/100\n",
            "575/575 [==============================] - 139s 242ms/step - loss: 4.3672 - accuracy: 0.1788\n",
            "Epoch 18/100\n",
            "575/575 [==============================] - 139s 242ms/step - loss: 4.3156 - accuracy: 0.1817\n",
            "Epoch 19/100\n",
            "575/575 [==============================] - 138s 240ms/step - loss: 4.2770 - accuracy: 0.1850\n",
            "Epoch 20/100\n",
            "575/575 [==============================] - 139s 241ms/step - loss: 4.2015 - accuracy: 0.1892\n",
            "Epoch 21/100\n",
            "575/575 [==============================] - 139s 242ms/step - loss: 4.1301 - accuracy: 0.1942\n",
            "Epoch 22/100\n",
            "575/575 [==============================] - 139s 242ms/step - loss: 4.0619 - accuracy: 0.1990\n",
            "Epoch 23/100\n",
            "575/575 [==============================] - 140s 243ms/step - loss: 4.0252 - accuracy: 0.2026\n",
            "Epoch 24/100\n",
            "575/575 [==============================] - 140s 243ms/step - loss: 3.9776 - accuracy: 0.2071\n",
            "Epoch 25/100\n",
            "575/575 [==============================] - 139s 241ms/step - loss: 3.9135 - accuracy: 0.2149\n",
            "Epoch 26/100\n",
            "575/575 [==============================] - 201s 349ms/step - loss: 3.8612 - accuracy: 0.2191\n",
            "Epoch 27/100\n",
            "575/575 [==============================] - 169s 294ms/step - loss: 3.7884 - accuracy: 0.2270\n",
            "Epoch 28/100\n",
            "575/575 [==============================] - 136s 237ms/step - loss: 3.7341 - accuracy: 0.2327\n",
            "Epoch 29/100\n",
            "575/575 [==============================] - 138s 239ms/step - loss: 3.6880 - accuracy: 0.2398\n",
            "Epoch 30/100\n",
            "575/575 [==============================] - 140s 243ms/step - loss: 3.6477 - accuracy: 0.2458\n",
            "Epoch 31/100\n",
            "575/575 [==============================] - 140s 244ms/step - loss: 3.6112 - accuracy: 0.2519\n",
            "Epoch 32/100\n",
            "575/575 [==============================] - 140s 243ms/step - loss: 3.5875 - accuracy: 0.2570\n",
            "Epoch 33/100\n",
            "575/575 [==============================] - 140s 244ms/step - loss: 3.5851 - accuracy: 0.2609\n",
            "Epoch 34/100\n",
            "575/575 [==============================] - 140s 243ms/step - loss: 3.5619 - accuracy: 0.2656\n",
            "Epoch 35/100\n",
            "575/575 [==============================] - 140s 244ms/step - loss: 3.4998 - accuracy: 0.2741\n",
            "Epoch 36/100\n",
            "575/575 [==============================] - 141s 245ms/step - loss: 3.4444 - accuracy: 0.2802\n",
            "Epoch 37/100\n",
            "575/575 [==============================] - 141s 244ms/step - loss: 3.5343 - accuracy: 0.2731\n",
            "Epoch 38/100\n",
            "575/575 [==============================] - 209s 364ms/step - loss: 3.4479 - accuracy: 0.2827\n",
            "Epoch 39/100\n",
            "575/575 [==============================] - 217s 378ms/step - loss: 3.3751 - accuracy: 0.2901\n",
            "Epoch 40/100\n",
            "575/575 [==============================] - 216s 376ms/step - loss: 3.3075 - accuracy: 0.3001\n",
            "Epoch 41/100\n",
            "575/575 [==============================] - 216s 376ms/step - loss: 3.4064 - accuracy: 0.2938\n",
            "Epoch 42/100\n",
            "575/575 [==============================] - 215s 374ms/step - loss: 3.3330 - accuracy: 0.3027\n",
            "Epoch 43/100\n",
            "575/575 [==============================] - 218s 379ms/step - loss: 3.3271 - accuracy: 0.3067\n",
            "Epoch 44/100\n",
            "575/575 [==============================] - 214s 372ms/step - loss: 3.2496 - accuracy: 0.3145\n",
            "Epoch 45/100\n",
            "575/575 [==============================] - 218s 378ms/step - loss: 3.2842 - accuracy: 0.3159\n",
            "Epoch 46/100\n",
            "575/575 [==============================] - 161s 280ms/step - loss: 3.2762 - accuracy: 0.3182\n",
            "Epoch 47/100\n",
            "575/575 [==============================] - 140s 244ms/step - loss: 3.2978 - accuracy: 0.3192\n",
            "Epoch 48/100\n",
            "575/575 [==============================] - 142s 247ms/step - loss: 3.1765 - accuracy: 0.3299\n",
            "Epoch 49/100\n",
            "575/575 [==============================] - 140s 243ms/step - loss: 3.1109 - accuracy: 0.3369\n",
            "Epoch 50/100\n",
            "575/575 [==============================] - 139s 241ms/step - loss: 3.0815 - accuracy: 0.3437\n",
            "Epoch 51/100\n",
            "575/575 [==============================] - 139s 242ms/step - loss: 3.0753 - accuracy: 0.3472\n",
            "Epoch 52/100\n",
            "575/575 [==============================] - 140s 243ms/step - loss: 2.9749 - accuracy: 0.3571\n",
            "Epoch 53/100\n",
            "575/575 [==============================] - 141s 245ms/step - loss: 3.0235 - accuracy: 0.3577\n",
            "Epoch 54/100\n",
            "575/575 [==============================] - 156s 272ms/step - loss: 3.0238 - accuracy: 0.3592\n",
            "Epoch 55/100\n",
            "575/575 [==============================] - 221s 383ms/step - loss: 2.9297 - accuracy: 0.3702\n",
            "Epoch 56/100\n",
            "575/575 [==============================] - 215s 374ms/step - loss: 2.8785 - accuracy: 0.3748\n",
            "Epoch 57/100\n",
            "575/575 [==============================] - 216s 375ms/step - loss: 2.8648 - accuracy: 0.3795\n",
            "Epoch 58/100\n",
            "575/575 [==============================] - 231s 401ms/step - loss: 3.7729 - accuracy: 0.2774\n",
            "Epoch 59/100\n",
            "575/575 [==============================] - 219s 381ms/step - loss: 3.6246 - accuracy: 0.2822\n",
            "Epoch 60/100\n",
            "575/575 [==============================] - 148s 257ms/step - loss: 3.5860 - accuracy: 0.2802\n",
            "Epoch 61/100\n",
            "575/575 [==============================] - 139s 241ms/step - loss: 3.5234 - accuracy: 0.2844\n",
            "Epoch 62/100\n",
            "575/575 [==============================] - 140s 243ms/step - loss: 3.4533 - accuracy: 0.2891\n",
            "Epoch 63/100\n",
            "575/575 [==============================] - 137s 238ms/step - loss: 3.3548 - accuracy: 0.2997\n",
            "Epoch 64/100\n",
            "575/575 [==============================] - 137s 239ms/step - loss: 3.3361 - accuracy: 0.3032\n",
            "Epoch 65/100\n",
            "575/575 [==============================] - 137s 239ms/step - loss: 3.3232 - accuracy: 0.3029\n",
            "Epoch 66/100\n",
            "575/575 [==============================] - 138s 240ms/step - loss: 3.2094 - accuracy: 0.3187\n",
            "Epoch 67/100\n",
            "575/575 [==============================] - 137s 238ms/step - loss: 3.1809 - accuracy: 0.3224\n",
            "Epoch 68/100\n",
            "575/575 [==============================] - 137s 239ms/step - loss: 3.1441 - accuracy: 0.3280\n",
            "Epoch 69/100\n",
            "575/575 [==============================] - 138s 240ms/step - loss: 3.2094 - accuracy: 0.3200\n",
            "Epoch 70/100\n",
            "575/575 [==============================] - 150s 261ms/step - loss: 3.0485 - accuracy: 0.3435\n",
            "Epoch 71/100\n",
            "575/575 [==============================] - 218s 380ms/step - loss: 2.9979 - accuracy: 0.3528\n",
            "Epoch 72/100\n",
            "575/575 [==============================] - 208s 362ms/step - loss: 2.9571 - accuracy: 0.3567\n",
            "Epoch 73/100\n",
            "575/575 [==============================] - 145s 252ms/step - loss: 2.9037 - accuracy: 0.3673\n",
            "Epoch 74/100\n",
            "575/575 [==============================] - 149s 260ms/step - loss: 2.9233 - accuracy: 0.3642\n",
            "Epoch 75/100\n",
            "116/575 [=====>........................] - ETA: 1:59 - loss: 3.4211 - accuracy: 0.2960"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/min-is/Chapman-University/CPSC 393 - Machine Learning/Homework/Homework 4/HW4_SP23.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://github/min-is/Chapman-University/CPSC%20393%20-%20Machine%20Learning/Homework/Homework%204/HW4_SP23.ipynb#ch0000012vscode-vfs?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(loss \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, optimizer \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, metrics \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell://github/min-is/Chapman-University/CPSC%20393%20-%20Machine%20Learning/Homework/Homework%204/HW4_SP23.ipynb#ch0000012vscode-vfs?line=3'>4</a>\u001b[0m \u001b[39m# fit model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://github/min-is/Chapman-University/CPSC%20393%20-%20Machine%20Learning/Homework/Homework%204/HW4_SP23.ipynb#ch0000012vscode-vfs?line=4'>5</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X, y, batch_size \u001b[39m=\u001b[39;49m \u001b[39m128\u001b[39;49m, epochs \u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m)\n",
            "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=61'>62</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=62'>63</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///usr/local/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=63'>64</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/keras/engine/training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/keras/engine/training.py?line=1376'>1377</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/keras/engine/training.py?line=1377'>1378</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/keras/engine/training.py?line=1378'>1379</a>\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/keras/engine/training.py?line=1379'>1380</a>\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/keras/engine/training.py?line=1380'>1381</a>\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/keras/engine/training.py?line=1381'>1382</a>\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/keras/engine/training.py?line=1382'>1383</a>\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> <a href='file:///usr/local/lib/python3.8/site-packages/keras/engine/training.py?line=1383'>1384</a>\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/keras/engine/training.py?line=1384'>1385</a>\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/keras/engine/training.py?line=1385'>1386</a>\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
            "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=147'>148</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=148'>149</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=149'>150</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=150'>151</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=151'>152</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=911'>912</a>\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=913'>914</a>\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=914'>915</a>\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=916'>917</a>\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=917'>918</a>\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
            "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=943'>944</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=944'>945</a>\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=945'>946</a>\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=946'>947</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=947'>948</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=948'>949</a>\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=949'>950</a>\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=950'>951</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
            "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=2952'>2953</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=2953'>2954</a>\u001b[0m   (graph_function,\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=2954'>2955</a>\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=2955'>2956</a>\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=2956'>2957</a>\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
            "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1848'>1849</a>\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1849'>1850</a>\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1850'>1851</a>\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1851'>1852</a>\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1852'>1853</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1853'>1854</a>\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1854'>1855</a>\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1855'>1856</a>\u001b[0m     args,\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1856'>1857</a>\u001b[0m     possible_gradient_type,\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1857'>1858</a>\u001b[0m     executing_eagerly)\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1858'>1859</a>\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
            "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=496'>497</a>\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=497'>498</a>\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=498'>499</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=499'>500</a>\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=500'>501</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=501'>502</a>\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=502'>503</a>\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=503'>504</a>\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=504'>505</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=505'>506</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=506'>507</a>\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=507'>508</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=510'>511</a>\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=511'>512</a>\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
            "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py?line=51'>52</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py?line=52'>53</a>\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py?line=53'>54</a>\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py?line=54'>55</a>\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py?line=55'>56</a>\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py?line=56'>57</a>\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# comile model\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "\n",
        "# fit model\n",
        "model.fit(X, y, batch_size = 128, epochs = 100)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
