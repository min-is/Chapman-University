{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 13:34:18.333750: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.datasets import mnist, fashion_mnist\n",
    "import tensorflow.keras as kb\n",
    "from tensorflow.keras import backend\n",
    "import tensorflow as tf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced CNN Techniques\n",
    "- Data Augmentation\n",
    "- Residual Connections\n",
    "- Depthwise Separable Connections\n",
    "- Visualizing Layer Activations\n",
    "- Visualizing Filters\n",
    "- Grad CAM\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "Data Augmentation is a method specific to CNNs that allows us to regularize our Network by showing the network the same image over and over with *slight* changes to things like:\n",
    "\n",
    "- rotation\n",
    "- crop\n",
    "- zoom\n",
    "- translation\n",
    "\n",
    "This is also helpful because we want our CNN to be able to recognize images even when they're shifted a little bit. \n",
    "\n",
    "Keras has a bunch of [`Random_*` layers](https://keras.io/api/layers/preprocessing_layers/image_augmentation/) which perform data augmentation for us. We can add a stack of them to a CNN before the first layer in order to perform data augmentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Load the data and split it between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = kb.utils.to_categorical(y_train, 10)\n",
    "y_test = kb.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 13:34:47.509988: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " random_flip (RandomFlip)    (None, 28, 28, 1)         0         \n",
      "                                                                 \n",
      " random_zoom (RandomZoom)    (None, 28, 28, 1)         0         \n",
      "                                                                 \n",
      " random_rotation (RandomRota  (None, 28, 28, 1)        0         \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 28, 28, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 12544)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               1254500   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,274,326\n",
      "Trainable params: 1,274,326\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = kb.Sequential([\n",
    "    kb.layers.InputLayer(input_shape =(28,28,1)),\n",
    "    kb.layers.RandomFlip(),\n",
    "    kb.layers.RandomZoom(0.2),\n",
    "    kb.layers.RandomRotation(0.1),\n",
    "    kb.layers.Conv2D(32, (3, 3), activation='relu', padding = \"same\"),\n",
    "    kb.layers.Conv2D(64, (3, 3), activation='relu', padding = \"same\"),\n",
    "    kb.layers.MaxPooling2D((2, 2)),\n",
    "    kb.layers.Flatten(),\n",
    "    kb.layers.Dense(100, activation='relu'),\n",
    "    kb.layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "1875/1875 [==============================] - 107s 55ms/step - loss: 0.5903 - accuracy: 0.7803 - val_loss: 0.4587 - val_accuracy: 0.8314\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 100s 53ms/step - loss: 0.4265 - accuracy: 0.8431 - val_loss: 0.4023 - val_accuracy: 0.8599\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 112s 60ms/step - loss: 0.3801 - accuracy: 0.8585 - val_loss: 0.4279 - val_accuracy: 0.8489\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 115s 61ms/step - loss: 0.3490 - accuracy: 0.8701 - val_loss: 0.3587 - val_accuracy: 0.8704\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 97s 52ms/step - loss: 0.3269 - accuracy: 0.8794 - val_loss: 0.3360 - val_accuracy: 0.8762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fccd1b0c730>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, validation_data = (x_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Connections\n",
    "\n",
    "Residual Connections create two paths for data to flow through:\n",
    "\n",
    "1. the traditional path which pushes the input image through the convolutional + pooling layers\n",
    "2. a non-destructive path that passes the image through unchanged (though we may reduce the size to match the output of the other path)\n",
    "\n",
    "These two paths are then added together before passing the output through to the next block of conv + pool layers. \n",
    "\n",
    "Because we're no longer using a simple, sequential stack of layers, we have to use the Functional API to define this architechture rather than `Sequential()`. ðŸ˜­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 28, 28, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 26, 26, 32)   320         ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 26, 26, 64)   18496       ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 26, 26, 64)   2112        ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 26, 26, 64)   0           ['conv2d_6[0][0]',               \n",
      "                                                                  'conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 13, 13, 64)  0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 10816)        0           ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 100)          1081700     ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 10)           1010        ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,103,638\n",
      "Trainable params: 1,103,638\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = kb.Input(shape = (28,28,1))\n",
    "x = kb.layers.Conv2D(32, (3,3), activation = \"relu\")(input)\n",
    "residual = x\n",
    "x = kb.layers.Conv2D(64, (3,3), padding = \"same\", activation = \"relu\")(x)\n",
    "residual = kb.layers.Conv2D(64,(1,1))(residual)\n",
    "x = kb.layers.add([x,residual])\n",
    "x = kb.layers.MaxPooling2D((2, 2))(x)\n",
    "x = kb.layers.Flatten()(x)\n",
    "x = kb.layers.Dense(100, activation='relu')(x)\n",
    "output = kb.layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = kb.Model(inputs = input, outputs = output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 77s 41ms/step - loss: 0.3527 - accuracy: 0.8734 - val_loss: 0.2677 - val_accuracy: 0.9026\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 88s 47ms/step - loss: 0.2174 - accuracy: 0.9189 - val_loss: 0.2259 - val_accuracy: 0.9148\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 100s 53ms/step - loss: 0.1621 - accuracy: 0.9406 - val_loss: 0.2371 - val_accuracy: 0.9177\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 101s 54ms/step - loss: 0.1196 - accuracy: 0.9552 - val_loss: 0.2693 - val_accuracy: 0.9119\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 101s 54ms/step - loss: 0.0876 - accuracy: 0.9673 - val_loss: 0.2957 - val_accuracy: 0.9170\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fccdb7d51f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, validation_data = (x_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depthwise Separable Convolutions\n",
    "\n",
    "[Depthwise Separable Convolutions](https://keras.io/api/layers/convolution_layers/separable_convolution2d/) are a way to reduce the computational cost of performing a convolution. DSC's apply a filter to each channel in an input image *separately* and then convolve those outputs together. This does assume that the different channels in the input are highly independent, but that's often true so it typically won't reduce the performance of your model unless that assumption is violated.\n",
    "\n",
    "Luckily, `SeparableConv2D()` is a drop-in replacement for `Conv2D()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " separable_conv2d_1 (Separab  (None, 28, 28, 32)       73        \n",
      " leConv2D)                                                       \n",
      "                                                                 \n",
      " separable_conv2d_2 (Separab  (None, 28, 28, 64)       2400      \n",
      " leConv2D)                                                       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 14, 14, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 12544)             0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 100)               1254500   \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,257,983\n",
      "Trainable params: 1,257,983\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = kb.Sequential([\n",
    "    kb.layers.InputLayer(input_shape =(28,28,1)),\n",
    "    kb.layers.SeparableConv2D(32, (3, 3), activation='relu', padding = \"same\"),\n",
    "    kb.layers.SeparableConv2D(64, (3, 3), activation='relu', padding = \"same\"),\n",
    "    kb.layers.MaxPooling2D((2, 2)),\n",
    "    kb.layers.Flatten(),\n",
    "    kb.layers.Dense(100, activation='relu'),\n",
    "    kb.layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 53s 28ms/step - loss: 0.1324 - accuracy: 0.9515 - val_loss: 0.2792 - val_accuracy: 0.9125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fccb6b78430>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=1, validation_data = (x_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Layer Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " c1 (Conv2D)                 (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " c2 (Conv2D)                 (None, 28, 28, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPoolin  (None, 14, 14, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_9 (Flatten)         (None, 12544)             0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 10)                125450    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 144,266\n",
      "Trainable params: 144,266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = kb.Sequential([\n",
    "    kb.layers.InputLayer(input_shape =(28,28,1)),\n",
    "    kb.layers.Conv2D(32, (3, 3), activation='relu', padding = \"same\", name = \"c1\"),\n",
    "    kb.layers.Conv2D(64, (3, 3), activation='relu', padding = \"same\", name = \"c2\"),\n",
    "    kb.layers.MaxPooling2D((2, 2)),\n",
    "    kb.layers.Flatten(),\n",
    "    kb.layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 74s 39ms/step - loss: 0.3747 - accuracy: 0.8674 - val_loss: 0.2898 - val_accuracy: 0.8972\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 237s 127ms/step - loss: 0.2463 - accuracy: 0.9119 - val_loss: 0.2623 - val_accuracy: 0.9077\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 27077s 14s/step - loss: 0.2017 - accuracy: 0.9275 - val_loss: 0.2661 - val_accuracy: 0.9068\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 18200s 10s/step - loss: 0.1698 - accuracy: 0.9392 - val_loss: 0.2370 - val_accuracy: 0.9147\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 12336s 7s/step - loss: 0.1432 - accuracy: 0.9484 - val_loss: 0.2529 - val_accuracy: 0.9155\n",
      "Epoch 6/10\n",
      "1670/1875 [=========================>....] - ETA: 10s - loss: 0.1233 - accuracy: 0.9564"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10, validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 28, 28, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 14, 14, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,816\n",
      "Trainable params: 18,816\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "layer_out = []\n",
    "layer_name = []\n",
    "\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, (kb.layers.Conv2D, kb.layers.MaxPooling2D)):\n",
    "        layer_out.append(layer.output)\n",
    "        layer_name.append(layer.name)\n",
    "\n",
    "layer_activations = kb.Model(inputs = model.input, outputs = layer_out)\n",
    "\n",
    "layer_activations.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 52ms/step\n"
     ]
    }
   ],
   "source": [
    "img = kb.utils.img_to_array(x_train[0])\n",
    "img = np.expand_dims(img, axis = 0)\n",
    "activations = layer_activations.predict(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fccdf89bcd0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# which layer and channel you want to look at\n",
    "layer = 0\n",
    "channel = 3\n",
    "\n",
    "# grab layer and channel and plot\n",
    "layer_act = activations[layer]\n",
    "plt.imshow(layer_act[0,:,:,channel], cmap = \"viridis\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Filters\n",
    "\n",
    "We can also visualize the filters learned by your CNN by using *gradient **ascent*** to find an image that *maximally* activates the filter as it's slid across an input. \n",
    "\n",
    "We start with a blank image and then make iterative changes that maximize the filter output more and more at each step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = model.get_layer(\"c1\")\n",
    "\n",
    "feature_extractor = kb.Model(inputs = model.input, outputs = layer.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(input_image, filter_index):\n",
    "    activation = feature_extractor(input_image)\n",
    "    # We avoid border artifacts by only involving non-border pixels in the loss.\n",
    "    filter_activation = activation[:, 2:-2, 2:-2, filter_index]\n",
    "    return tf.reduce_mean(filter_activation)\n",
    "\n",
    "@tf.function\n",
    "def gradient_ascent_step(img, filter_index, learning_rate):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(img)\n",
    "        loss = compute_loss(img, filter_index)\n",
    "    # Compute gradients.\n",
    "    grads = tape.gradient(loss, img)\n",
    "    # Normalize gradients.\n",
    "    grads = tf.math.l2_normalize(grads)\n",
    "    img += learning_rate * grads\n",
    "    return loss, img\n",
    "\n",
    "\n",
    "def initialize_image():\n",
    "    # We start from a gray image with some random noise\n",
    "    img = tf.random.uniform((1, 28, 28, 1))\n",
    "    # ResNet50V2 expects inputs in the range [-1, +1].\n",
    "    # Here we scale our random inputs to [-0.125, +0.125]\n",
    "    return (img - 0.5) * 0.25\n",
    "\n",
    "\n",
    "def visualize_filter(filter_index):\n",
    "    # We run gradient ascent for 20 steps\n",
    "    iterations = 30\n",
    "    learning_rate = 10.0\n",
    "    img = initialize_image()\n",
    "    for iteration in range(iterations):\n",
    "        loss, img = gradient_ascent_step(img, filter_index, learning_rate)\n",
    "\n",
    "    # Decode the resulting input image\n",
    "    img = deprocess_image(img[0].numpy())\n",
    "    return loss, img\n",
    "\n",
    "\n",
    "def deprocess_image(img):\n",
    "    # Normalize array: center on 0., ensure variance is 0.15\n",
    "    img -= img.mean()\n",
    "    img /= img.std() + 1e-5\n",
    "    img *= 0.15\n",
    "\n",
    "    # Center crop\n",
    "    img = img[2:-2, 2:-2, :]\n",
    "\n",
    "    # Clip to [0, 1]\n",
    "    img += 0.5\n",
    "    img = np.clip(img, 0, 1)\n",
    "\n",
    "    # Convert to RGB array\n",
    "    img *= 255\n",
    "    img = np.clip(img, 0, 255).astype(\"uint8\")\n",
    "    return img\n",
    "    image -= image.mean()\n",
    "    image /= image.std()\n",
    "    image *= 64\n",
    "    image += 128\n",
    "    image = np.clip(image, 0, 255).astype(\"uint8\")\n",
    "    image = image[25:-25, 25:-25, :]\n",
    "    return(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAAAAADFHGIkAAACY0lEQVR4nAFYAqf9AYoH8Q/r53pnS0Zvi3VLJbVefYuWJOtVuwEU0jOLENTDXbUo/BrCSpGbkyPxRk2XA9IClHCS92FgzR8lp1TITfQho/QT0vKf1rosAPAa5jh8fKE49DubMd84vAzGoCNXzaAAxAFdTqwOZXyPlRrbBGqWAmLBt3PbEpl4/MwCtzrCQgEMX3XyAkSv1TvhFBeBSOlEfSkFAWIgIbgrGdPxFe49rSoaB8JnbYlTubyqpgJFtlHg0QtjvRxdm6XCB7sMAjrKk5MuO74CCDlxaAMAe3O2G/mDRSS6W6E5sVQjWmL0Aqcy2y4kgmIgQG5zv1LFVgKpIkt55ZrQ0QHVnQO7wmwbwaQ60HMb5D2/VJCch/OaTmABhOF4pOoZ1SM0zeJqYzOgGq24ziF0o76NAaXQusJKHJJSPUppVTS731qPnsyiM+MbJwA/9D6XO9NjpziwrE5Z/RzJgC2wzStf4YQCO1hpwGqp1TcmhAZlEXWAbnUug+Wf0tnKAlkRE8pAv0zSHyYBW1zxEzN6Zfyl/TrIUQFAtERlulPI2DwCKK1pivsv9TsCri4VmpgCPHYMVcLYhs9gs+gYoRN/CACYSOLPUCSlAfFHYb1Ssiv8+Se2a1O1eq5p9R0TH9oM8QJuLjIH9A3KTMpU4cZ08eyGAnnZNi608y0AN75G9RjHaclQU8kKx2WJW82oOphdZexXAY8kw9U+NXJIR02YCM7Ahcn81RtHVrafPwLrenTVPaVZAsWqkynhQBTnhCtfspKx1RcCVDaBcN/RGzaxDCx+dumhEnr+zc4jymbQjM4VEC/caTsAAAAASUVORK5CYII=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "loss, img = visualize_filter(7)\n",
    "kb.preprocessing.image.save_img(\"0.png\", img)\n",
    "display(Image(\"0.png\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Class Activation Heatmaps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Display\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fccaa141e80>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model_builder = keras.applications.xception.Xception\n",
    "# img_size = (299, 299)\n",
    "# preprocess_input = keras.applications.xception.preprocess_input\n",
    "# decode_predictions = keras.applications.xception.decode_predictions\n",
    "\n",
    "img_size = (28, 28)\n",
    "last_conv_layer_name = \"conv2d_15\"\n",
    "\n",
    "img = np.expand_dims(x_train[0], 0)\n",
    "img.shape\n",
    "\n",
    "plt.imshow(img.reshape((28,28)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_img_array(img, size):\n",
    "    # `array` is a float32 Numpy array of shape (28, 28, 1)\n",
    "    array = keras.preprocessing.image.img_to_array(img)\n",
    "    # We add a dimension to transform our array into a \"batch\"\n",
    "    array = np.expand_dims(array, axis=0)\n",
    "    return array\n",
    "\n",
    "\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    # First, we create a model that maps the input image to the activations\n",
    "    # of the last conv layer as well as the output predictions\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    # Then, we compute the gradient of the top predicted class for our input image\n",
    "    # with respect to the activations of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    # This is the gradient of the output neuron (top predicted or chosen)\n",
    "    # with regard to the output feature map of the last conv layer\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "\n",
    "    # This is a vector where each entry is the mean intensity of the gradient\n",
    "    # over a specific feature map channel\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1))\n",
    "\n",
    "    # We multiply each channel in the feature map array\n",
    "    # by \"how important this channel is\" with regard to the top predicted class\n",
    "    # then sum all the channels to obtain the heatmap class activation\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remove last layer's softmax\n",
    "model.layers[-1].activation = None\n",
    "\n",
    "# Print what the top predicted class is\n",
    "preds = model.predict(img)\n",
    "\n",
    "# Generate class activation heatmap\n",
    "heatmap = make_gradcam_heatmap(img, model, last_conv_layer_name)\n",
    "\n",
    "# Display heatmap\n",
    "plt.matshow(heatmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# def save_and_display_gradcam(img, heatmap, cam_path=\"cam.jpg\", alpha=0.4):\n",
    "#     # Rescale heatmap to a range 0-255\n",
    "#     heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "#     # Use jet colormap to colorize heatmap\n",
    "#     jet = cm.get_cmap(\"jet\")\n",
    "\n",
    "#     # Use RGB values of the colormap\n",
    "#     jet_colors = jet(np.arange(256))[:, :1]\n",
    "#     jet_heatmap = jet_colors[heatmap]\n",
    "\n",
    "#     # Create an image with RGB colorized heatmap\n",
    "#     jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n",
    "#     jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
    "#     jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n",
    "\n",
    "#     # Superimpose the heatmap on original image\n",
    "#     superimposed_img = jet_heatmap * alpha + img\n",
    "#     plt.matshow(superimposed_img.reshape((28,28)))\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# save_and_display_gradcam(img, heatmap, alpha = 0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
