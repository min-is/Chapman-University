{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2OZT6wL4xIFs",
        "outputId": "6300fcfd-36d8-4f1f-c289-0895a24b9650"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from keras.datasets import mnist\n",
        "import tensorflow.keras as kb\n",
        "from tensorflow.keras import backend\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "\n",
        "from plotnine import *\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, roc_auc_score\n",
        "\n",
        "from sklearn.linear_model import LinearRegression # Linear Regression Model\n",
        "from sklearn.preprocessing import StandardScaler #Z-score variables\n",
        "\n",
        "from sklearn.model_selection import train_test_split # simple TT split cv\n",
        "from sklearn.model_selection import KFold # k-fold cv\n",
        "from sklearn.model_selection import LeaveOneOut #LOO cv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hPEQ6HJxIFu"
      },
      "source": [
        "# Together\n",
        "Neural Networks are more of a framework for building custom models than a specific type of model. When building a neural network you need to think about 3 main things:\n",
        "\n",
        "1. The structure of your model (how many layers? how many nodes? what activation functions?)\n",
        "2. The loss function for your model\n",
        "3. The optimizer you'll use to train (we'll get to that in the next NN lecture)\n",
        "\n",
        "Your input and output layers are pre-determined by the number and type of input you want to go into your model, and the form of output that you want. For example for a regression problem you likely want a single node in the output layer. For a multi-class classification problem, you want `n` nodes in the output layer, one for each possible output category. \n",
        "\n",
        "\n",
        "## Review\n",
        "\n",
        "### Layers, Hidden Layers, Nodes\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1iN4PCbZyIDQCgKx9odEpTaXpHOc-vZzg\" alt=\"Q\" width = \"400\"/>\n",
        "\n",
        "### Calculating the Value of a Node\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1P_jfOCXs4wFUVpyFeRH3ysAIaeoHSe54\" alt=\"Q\" width = \"400\"/>\n",
        "\n",
        "### Activation Functions\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1CXSZZ9NjMlvdtopwGD5mAlFMukAmPBrQ\" alt=\"Q\" width = \"400\"/>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1I9vs6IgnHlohBkTt0G7E-vHLBQTT169_\" alt=\"Q\" width = \"400\"/>\n",
        "\n",
        "Softmax:\n",
        "$$ f(x) = \\frac{e^{z_i}}{\\sum_{j=1}^N e^z_j}$$\n",
        "\n",
        "The softmax activation creates a vector of probabilities that sum to 1, and as a bonus, it's ✨differentiable✨.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6_JHdD7-xIFw",
        "outputId": "9cff0fe9-e06a-48d0-da70-15d5c9f0d08b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([0.001, 0.089, 0.243, 0.661, 0.004, 0.002])"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = np.array([1,6,7,8,3,2])\n",
        "soft_x = np.exp(x)\n",
        "soft_x = soft_x/np.sum(soft_x)\n",
        "print(np.sum(soft_x))\n",
        "np.round(soft_x,3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W07zJ8WMxIFw"
      },
      "source": [
        "\n",
        "\n",
        "## Building a Simple NN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2pPxv2xKxIFx"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>artist_name</th>\n",
              "      <th>danceability</th>\n",
              "      <th>energy</th>\n",
              "      <th>key</th>\n",
              "      <th>loudness</th>\n",
              "      <th>mode</th>\n",
              "      <th>speechiness</th>\n",
              "      <th>acousticness</th>\n",
              "      <th>instrumentalness</th>\n",
              "      <th>liveness</th>\n",
              "      <th>valence</th>\n",
              "      <th>duration_ms</th>\n",
              "      <th>track_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Jonas Brothers</td>\n",
              "      <td>0.594</td>\n",
              "      <td>0.464</td>\n",
              "      <td>0</td>\n",
              "      <td>-7.898</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0340</td>\n",
              "      <td>0.306000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.1870</td>\n",
              "      <td>0.545</td>\n",
              "      <td>193866</td>\n",
              "      <td>Please Be Mine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Jonas Brothers</td>\n",
              "      <td>0.582</td>\n",
              "      <td>0.697</td>\n",
              "      <td>7</td>\n",
              "      <td>-3.842</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0695</td>\n",
              "      <td>0.000745</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.1850</td>\n",
              "      <td>0.701</td>\n",
              "      <td>153346</td>\n",
              "      <td>S.O.S.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Jonas Brothers</td>\n",
              "      <td>0.442</td>\n",
              "      <td>0.961</td>\n",
              "      <td>1</td>\n",
              "      <td>-2.914</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0928</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.000036</td>\n",
              "      <td>0.0619</td>\n",
              "      <td>0.405</td>\n",
              "      <td>168480</td>\n",
              "      <td>Mandy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Jonas Brothers</td>\n",
              "      <td>0.659</td>\n",
              "      <td>0.857</td>\n",
              "      <td>11</td>\n",
              "      <td>-5.850</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0437</td>\n",
              "      <td>0.004500</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.3350</td>\n",
              "      <td>0.798</td>\n",
              "      <td>201960</td>\n",
              "      <td>Year 3000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Jonas Brothers</td>\n",
              "      <td>0.468</td>\n",
              "      <td>0.893</td>\n",
              "      <td>11</td>\n",
              "      <td>-3.554</td>\n",
              "      <td>1</td>\n",
              "      <td>0.1300</td>\n",
              "      <td>0.003890</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.3670</td>\n",
              "      <td>0.810</td>\n",
              "      <td>164973</td>\n",
              "      <td>Hold On</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0     artist_name  danceability  energy  key  loudness  mode  \\\n",
              "0           1  Jonas Brothers         0.594   0.464    0    -7.898     1   \n",
              "1           2  Jonas Brothers         0.582   0.697    7    -3.842     1   \n",
              "2           3  Jonas Brothers         0.442   0.961    1    -2.914     1   \n",
              "3           4  Jonas Brothers         0.659   0.857   11    -5.850     1   \n",
              "4           5  Jonas Brothers         0.468   0.893   11    -3.554     1   \n",
              "\n",
              "   speechiness  acousticness  instrumentalness  liveness  valence  \\\n",
              "0       0.0340      0.306000          0.000000    0.1870    0.545   \n",
              "1       0.0695      0.000745          0.000000    0.1850    0.701   \n",
              "2       0.0928      0.000035          0.000036    0.0619    0.405   \n",
              "3       0.0437      0.004500          0.000002    0.3350    0.798   \n",
              "4       0.1300      0.003890          0.000000    0.3670    0.810   \n",
              "\n",
              "   duration_ms      track_name  \n",
              "0       193866  Please Be Mine  \n",
              "1       153346          S.O.S.  \n",
              "2       168480           Mandy  \n",
              "3       201960       Year 3000  \n",
              "4       164973         Hold On  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/Music_data.csv\")\n",
        "feats = [\"danceability\", \"energy\", \"loudness\",\"acousticness\", \"instrumentalness\", \"liveness\", \"duration_ms\"]\n",
        "predict = \"valence\"\n",
        "\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "09gw1YGGxIFx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 1.1342 - val_loss: 0.2911\n",
            "Epoch 2/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.1568 - val_loss: 0.0802\n",
            "Epoch 3/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.0606 - val_loss: 0.0429\n",
            "Epoch 4/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.0414 - val_loss: 0.0348\n",
            "Epoch 5/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0367 - val_loss: 0.0327\n",
            "Epoch 6/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0353 - val_loss: 0.0324\n",
            "Epoch 7/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0349 - val_loss: 0.0322\n",
            "Epoch 8/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0347 - val_loss: 0.0323\n",
            "Epoch 9/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0346 - val_loss: 0.0324\n",
            "Epoch 10/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 11/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0346 - val_loss: 0.0325\n",
            "Epoch 12/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0324\n",
            "Epoch 13/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0324\n",
            "Epoch 14/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 15/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 16/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 17/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 18/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0324\n",
            "Epoch 19/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 20/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 21/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0324\n",
            "Epoch 22/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0326\n",
            "Epoch 23/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 24/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0326\n",
            "Epoch 25/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 26/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 27/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0324\n",
            "Epoch 28/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 29/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0326\n",
            "Epoch 30/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0324\n",
            "Epoch 31/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 32/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0324\n",
            "Epoch 33/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0324\n",
            "Epoch 34/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 35/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 36/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0326\n",
            "Epoch 37/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 38/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0326\n",
            "Epoch 39/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 40/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0324\n",
            "Epoch 41/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 42/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 43/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 44/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 45/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0326\n",
            "Epoch 46/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 47/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 48/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0323\n",
            "Epoch 49/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0324\n",
            "Epoch 50/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0324\n",
            "Epoch 51/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0324\n",
            "Epoch 52/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0324\n",
            "Epoch 53/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 54/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 55/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 56/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 57/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0324\n",
            "Epoch 58/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0324\n",
            "Epoch 59/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 60/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0324\n",
            "Epoch 61/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0324\n",
            "Epoch 62/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0346 - val_loss: 0.0325\n",
            "Epoch 63/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 64/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0324\n",
            "Epoch 65/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 66/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 67/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 68/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0324\n",
            "Epoch 69/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 70/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0326\n",
            "Epoch 71/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 72/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0326\n",
            "Epoch 73/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0324\n",
            "Epoch 74/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0326\n",
            "Epoch 75/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0326\n",
            "Epoch 76/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0326\n",
            "Epoch 77/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 78/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0324\n",
            "Epoch 79/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0326\n",
            "Epoch 80/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0326\n",
            "Epoch 81/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 82/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0326\n",
            "Epoch 83/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 84/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0324\n",
            "Epoch 85/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0323\n",
            "Epoch 86/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0324\n",
            "Epoch 87/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0328\n",
            "Epoch 88/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0326\n",
            "Epoch 89/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0326\n",
            "Epoch 90/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0327\n",
            "Epoch 91/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 92/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 93/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0327\n",
            "Epoch 94/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0324\n",
            "Epoch 95/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0324\n",
            "Epoch 96/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0326\n",
            "Epoch 97/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 98/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 99/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.0325\n",
            "Epoch 100/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0324\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x15bc3f370>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Regression\n",
        "\n",
        "X = df[feats]\n",
        "y = df[\"valence\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2)\n",
        "\n",
        "z = StandardScaler()\n",
        "X_train[feats] = z.fit_transform(X_train[feats])\n",
        "X_test[feats] = z.transform(X_test[feats])\n",
        "\n",
        "#structure of the model\n",
        "\n",
        "model = kb.Sequential([\n",
        "    kb.layers.Dense(1, input_shape = [7]),\n",
        "])\n",
        "\n",
        "#how to train the model\n",
        "\n",
        "model.compile(loss = \"mean_squared_error\",\n",
        "    optimizer = kb.optimizers.SGD())\n",
        "\n",
        "#fit the model (same as SKlearn)\n",
        "\n",
        "model.fit(X_train, y_train, epochs = 100, validation_data = (X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SotJVPNKxIFx"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[array([[ 0.1078383 ],\n",
              "        [ 0.09139112],\n",
              "        [-0.01775693],\n",
              "        [ 0.02858892],\n",
              "        [-0.00499878],\n",
              "        [-0.00615598],\n",
              "        [-0.03469113]], dtype=float32),\n",
              " array([0.4728315], dtype=float32)]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# get weights from Neural Network\n",
        "model.get_weights()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeKL9A0kxIFy"
      },
      "source": [
        "# In Class\n",
        "\n",
        "## Building a Logistic Regression NN\n",
        "Let's change the model we built together so that it predicts `mode` instead of valence. Because we're now predicting a categorical variable (or rather, we're predicting the probability that the data is in a given category), we need to change the last layer of our network. Copy and paste the model we made together, and let's make some changes. \n",
        "\n",
        "- Change your `y` to be `mode` \n",
        "- Add a `activation = \"sigmoid\"` argument to your Layer\n",
        "- Change the `loss` argument in your `model.compile()` function to be `\"binary_crossentropy\"` (remember binary cross entropy is another name for log loss which we learned about in 392)\n",
        "\n",
        "\n",
        "Then fit your model and see how well it does!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XZ9m7uDFxIFy"
      },
      "outputs": [],
      "source": [
        "# change y and redo TTS\n",
        "\n",
        "X = df[feats]\n",
        "y = df[\"mode\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2)\n",
        "\n",
        "z = StandardScaler()\n",
        "X_train[feats] = z.fit_transform(X_train[feats])\n",
        "X_test[feats] = z.transform(X_test[feats])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "1PvcYGvpxIFy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "64/64 [==============================] - 1s 4ms/step - loss: 0.8931 - val_loss: 0.8127\n",
            "Epoch 2/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.8355 - val_loss: 0.7739\n",
            "Epoch 3/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.7901 - val_loss: 0.7433\n",
            "Epoch 4/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.7544 - val_loss: 0.7190\n",
            "Epoch 5/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.7261 - val_loss: 0.6995\n",
            "Epoch 6/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.7040 - val_loss: 0.6839\n",
            "Epoch 7/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6865 - val_loss: 0.6715\n",
            "Epoch 8/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6729 - val_loss: 0.6616\n",
            "Epoch 9/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6621 - val_loss: 0.6534\n",
            "Epoch 10/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6537 - val_loss: 0.6468\n",
            "Epoch 11/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6469 - val_loss: 0.6414\n",
            "Epoch 12/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6415 - val_loss: 0.6370\n",
            "Epoch 13/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6371 - val_loss: 0.6332\n",
            "Epoch 14/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6334 - val_loss: 0.6302\n",
            "Epoch 15/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6304 - val_loss: 0.6276\n",
            "Epoch 16/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6279 - val_loss: 0.6255\n",
            "Epoch 17/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6256 - val_loss: 0.6235\n",
            "Epoch 18/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6238 - val_loss: 0.6218\n",
            "Epoch 19/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6221 - val_loss: 0.6203\n",
            "Epoch 20/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6207 - val_loss: 0.6189\n",
            "Epoch 21/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6195 - val_loss: 0.6179\n",
            "Epoch 22/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6184 - val_loss: 0.6169\n",
            "Epoch 23/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6174 - val_loss: 0.6160\n",
            "Epoch 24/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6166 - val_loss: 0.6153\n",
            "Epoch 25/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6158 - val_loss: 0.6146\n",
            "Epoch 26/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6151 - val_loss: 0.6140\n",
            "Epoch 27/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6145 - val_loss: 0.6135\n",
            "Epoch 28/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6140 - val_loss: 0.6130\n",
            "Epoch 29/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6135 - val_loss: 0.6127\n",
            "Epoch 30/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6130 - val_loss: 0.6123\n",
            "Epoch 31/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6126 - val_loss: 0.6119\n",
            "Epoch 32/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6123 - val_loss: 0.6115\n",
            "Epoch 33/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6120 - val_loss: 0.6112\n",
            "Epoch 34/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6116 - val_loss: 0.6112\n",
            "Epoch 35/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6113 - val_loss: 0.6110\n",
            "Epoch 36/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6111 - val_loss: 0.6109\n",
            "Epoch 37/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6108 - val_loss: 0.6107\n",
            "Epoch 38/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6106 - val_loss: 0.6105\n",
            "Epoch 39/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6104 - val_loss: 0.6105\n",
            "Epoch 40/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6103 - val_loss: 0.6104\n",
            "Epoch 41/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6101 - val_loss: 0.6101\n",
            "Epoch 42/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6099 - val_loss: 0.6099\n",
            "Epoch 43/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6098 - val_loss: 0.6098\n",
            "Epoch 44/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6097 - val_loss: 0.6097\n",
            "Epoch 45/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6095 - val_loss: 0.6097\n",
            "Epoch 46/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6094 - val_loss: 0.6095\n",
            "Epoch 47/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6093 - val_loss: 0.6094\n",
            "Epoch 48/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6092 - val_loss: 0.6094\n",
            "Epoch 49/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6091 - val_loss: 0.6094\n",
            "Epoch 50/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6091 - val_loss: 0.6093\n",
            "Epoch 51/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6089 - val_loss: 0.6093\n",
            "Epoch 52/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6089 - val_loss: 0.6093\n",
            "Epoch 53/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.6088 - val_loss: 0.6093\n",
            "Epoch 54/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6088 - val_loss: 0.6093\n",
            "Epoch 55/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6087 - val_loss: 0.6092\n",
            "Epoch 56/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6086 - val_loss: 0.6091\n",
            "Epoch 57/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6086 - val_loss: 0.6091\n",
            "Epoch 58/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6085 - val_loss: 0.6090\n",
            "Epoch 59/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6085 - val_loss: 0.6091\n",
            "Epoch 60/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6084 - val_loss: 0.6090\n",
            "Epoch 61/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6084 - val_loss: 0.6090\n",
            "Epoch 62/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6084 - val_loss: 0.6089\n",
            "Epoch 63/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6083 - val_loss: 0.6090\n",
            "Epoch 64/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6083 - val_loss: 0.6091\n",
            "Epoch 65/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6083 - val_loss: 0.6090\n",
            "Epoch 66/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6082 - val_loss: 0.6090\n",
            "Epoch 67/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6082 - val_loss: 0.6090\n",
            "Epoch 68/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6082 - val_loss: 0.6090\n",
            "Epoch 69/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6082 - val_loss: 0.6090\n",
            "Epoch 70/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6081 - val_loss: 0.6089\n",
            "Epoch 71/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6081 - val_loss: 0.6089\n",
            "Epoch 72/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6081 - val_loss: 0.6089\n",
            "Epoch 73/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6081 - val_loss: 0.6089\n",
            "Epoch 74/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6080 - val_loss: 0.6089\n",
            "Epoch 75/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6080 - val_loss: 0.6089\n",
            "Epoch 76/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6080 - val_loss: 0.6089\n",
            "Epoch 77/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6080 - val_loss: 0.6089\n",
            "Epoch 78/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6080 - val_loss: 0.6089\n",
            "Epoch 79/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6080 - val_loss: 0.6089\n",
            "Epoch 80/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6079 - val_loss: 0.6087\n",
            "Epoch 81/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6079 - val_loss: 0.6087\n",
            "Epoch 82/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6080 - val_loss: 0.6086\n",
            "Epoch 83/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6079 - val_loss: 0.6088\n",
            "Epoch 84/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6079 - val_loss: 0.6089\n",
            "Epoch 85/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6079 - val_loss: 0.6090\n",
            "Epoch 86/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6079 - val_loss: 0.6090\n",
            "Epoch 87/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6078 - val_loss: 0.6090\n",
            "Epoch 88/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6078 - val_loss: 0.6089\n",
            "Epoch 89/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6079 - val_loss: 0.6088\n",
            "Epoch 90/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6078 - val_loss: 0.6089\n",
            "Epoch 91/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6078 - val_loss: 0.6088\n",
            "Epoch 92/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6078 - val_loss: 0.6088\n",
            "Epoch 93/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6078 - val_loss: 0.6088\n",
            "Epoch 94/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6078 - val_loss: 0.6090\n",
            "Epoch 95/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6078 - val_loss: 0.6090\n",
            "Epoch 96/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6078 - val_loss: 0.6090\n",
            "Epoch 97/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6077 - val_loss: 0.6090\n",
            "Epoch 98/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6077 - val_loss: 0.6090\n",
            "Epoch 99/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6077 - val_loss: 0.6090\n",
            "Epoch 100/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6077 - val_loss: 0.6091\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x15c456fa0>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#structure of the model\n",
        "\n",
        "model = kb.Sequential([\n",
        "    kb.layers.Dense(1, input_shape=[7], activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "#how to train the model\n",
        "\n",
        "model.compile(loss = \"binary_crossentropy\",\n",
        "    optimizer = kb.optimizers.SGD())\n",
        "\n",
        "#fit the model (same as SKlearn)\n",
        "model.fit(X_train, y_train, epochs = 100, validation_data = (X_test, y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Nx_ZW3sxIFy"
      },
      "source": [
        "## Making that Model DEEP\n",
        "\n",
        "Now, let's modify your model once more, this time to add hidden layers.\n",
        "\n",
        "- Your first `Dense()` layer should still have an `input_shape` of `[7]` but now you can make that hidden layer ANY SIZE you want\n",
        "- add at LEAST 3 more `Dense()` layers with however many nodes you want (play around with it)\n",
        "- make sure your last `Dense()` layer has an output dimension of `1` and a `sigmoid` activation\n",
        "- compile and fit your model\n",
        "\n",
        "### Question\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" alt=\"Q\" width = \"200\"/>\n",
        "\n",
        "How does the performance of your model change from the very simple NN you built before? Is there any overfitting going on?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "zjQXfDypxIFz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "64/64 [==============================] - 1s 4ms/step - loss: 0.6835 - val_loss: 0.6629\n",
            "Epoch 2/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6545 - val_loss: 0.6414\n",
            "Epoch 3/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6403 - val_loss: 0.6302\n",
            "Epoch 4/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6333 - val_loss: 0.6246\n",
            "Epoch 5/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6299 - val_loss: 0.6215\n",
            "Epoch 6/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6281 - val_loss: 0.6198\n",
            "Epoch 7/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6272 - val_loss: 0.6187\n",
            "Epoch 8/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6267 - val_loss: 0.6182\n",
            "Epoch 9/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6264 - val_loss: 0.6177\n",
            "Epoch 10/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6263 - val_loss: 0.6175\n",
            "Epoch 11/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6262 - val_loss: 0.6174\n",
            "Epoch 12/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6262 - val_loss: 0.6173\n",
            "Epoch 13/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6262 - val_loss: 0.6173\n",
            "Epoch 14/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6262 - val_loss: 0.6172\n",
            "Epoch 15/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6262 - val_loss: 0.6172\n",
            "Epoch 16/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6261 - val_loss: 0.6172\n",
            "Epoch 17/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6261 - val_loss: 0.6171\n",
            "Epoch 18/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6261 - val_loss: 0.6171\n",
            "Epoch 19/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6261 - val_loss: 0.6171\n",
            "Epoch 20/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6261 - val_loss: 0.6171\n",
            "Epoch 21/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6261 - val_loss: 0.6171\n",
            "Epoch 22/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6261 - val_loss: 0.6171\n",
            "Epoch 23/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6261 - val_loss: 0.6171\n",
            "Epoch 24/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6261 - val_loss: 0.6171\n",
            "Epoch 25/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6261 - val_loss: 0.6171\n",
            "Epoch 26/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6261 - val_loss: 0.6171\n",
            "Epoch 27/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6261 - val_loss: 0.6171\n",
            "Epoch 28/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6260 - val_loss: 0.6170\n",
            "Epoch 29/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6260 - val_loss: 0.6170\n",
            "Epoch 30/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6260 - val_loss: 0.6170\n",
            "Epoch 31/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6260 - val_loss: 0.6170\n",
            "Epoch 32/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6260 - val_loss: 0.6171\n",
            "Epoch 33/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6260 - val_loss: 0.6171\n",
            "Epoch 34/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6260 - val_loss: 0.6170\n",
            "Epoch 35/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6260 - val_loss: 0.6170\n",
            "Epoch 36/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6260 - val_loss: 0.6170\n",
            "Epoch 37/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6260 - val_loss: 0.6170\n",
            "Epoch 38/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6260 - val_loss: 0.6170\n",
            "Epoch 39/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6260 - val_loss: 0.6170\n",
            "Epoch 40/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6260 - val_loss: 0.6170\n",
            "Epoch 41/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6259 - val_loss: 0.6170\n",
            "Epoch 42/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6260 - val_loss: 0.6170\n",
            "Epoch 43/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6259 - val_loss: 0.6170\n",
            "Epoch 44/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6259 - val_loss: 0.6170\n",
            "Epoch 45/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6259 - val_loss: 0.6170\n",
            "Epoch 46/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6259 - val_loss: 0.6169\n",
            "Epoch 47/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6259 - val_loss: 0.6169\n",
            "Epoch 48/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6259 - val_loss: 0.6169\n",
            "Epoch 49/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6259 - val_loss: 0.6169\n",
            "Epoch 50/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6259 - val_loss: 0.6170\n",
            "Epoch 51/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6259 - val_loss: 0.6169\n",
            "Epoch 52/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6259 - val_loss: 0.6170\n",
            "Epoch 53/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6258 - val_loss: 0.6169\n",
            "Epoch 54/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6259 - val_loss: 0.6170\n",
            "Epoch 55/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6258 - val_loss: 0.6170\n",
            "Epoch 56/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6258 - val_loss: 0.6169\n",
            "Epoch 57/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6258 - val_loss: 0.6169\n",
            "Epoch 58/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6258 - val_loss: 0.6169\n",
            "Epoch 59/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6258 - val_loss: 0.6169\n",
            "Epoch 60/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6258 - val_loss: 0.6169\n",
            "Epoch 61/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6258 - val_loss: 0.6169\n",
            "Epoch 62/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6258 - val_loss: 0.6169\n",
            "Epoch 63/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6258 - val_loss: 0.6169\n",
            "Epoch 64/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6258 - val_loss: 0.6169\n",
            "Epoch 65/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6258 - val_loss: 0.6169\n",
            "Epoch 66/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6257 - val_loss: 0.6169\n",
            "Epoch 67/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.6257 - val_loss: 0.6169\n",
            "Epoch 68/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6257 - val_loss: 0.6169\n",
            "Epoch 69/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6257 - val_loss: 0.6169\n",
            "Epoch 70/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6257 - val_loss: 0.6169\n",
            "Epoch 71/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6257 - val_loss: 0.6169\n",
            "Epoch 72/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6257 - val_loss: 0.6169\n",
            "Epoch 73/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6257 - val_loss: 0.6169\n",
            "Epoch 74/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6257 - val_loss: 0.6169\n",
            "Epoch 75/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6257 - val_loss: 0.6169\n",
            "Epoch 76/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6257 - val_loss: 0.6169\n",
            "Epoch 77/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6256 - val_loss: 0.6169\n",
            "Epoch 78/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6257 - val_loss: 0.6168\n",
            "Epoch 79/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6257 - val_loss: 0.6168\n",
            "Epoch 80/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6257 - val_loss: 0.6168\n",
            "Epoch 81/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6256 - val_loss: 0.6168\n",
            "Epoch 82/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6256 - val_loss: 0.6168\n",
            "Epoch 83/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6256 - val_loss: 0.6168\n",
            "Epoch 84/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6256 - val_loss: 0.6168\n",
            "Epoch 85/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6256 - val_loss: 0.6168\n",
            "Epoch 86/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6256 - val_loss: 0.6168\n",
            "Epoch 87/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6256 - val_loss: 0.6168\n",
            "Epoch 88/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6256 - val_loss: 0.6168\n",
            "Epoch 89/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6256 - val_loss: 0.6168\n",
            "Epoch 90/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6256 - val_loss: 0.6168\n",
            "Epoch 91/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6256 - val_loss: 0.6168\n",
            "Epoch 92/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6255 - val_loss: 0.6168\n",
            "Epoch 93/100\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6256 - val_loss: 0.6168\n",
            "Epoch 94/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6255 - val_loss: 0.6168\n",
            "Epoch 95/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6255 - val_loss: 0.6168\n",
            "Epoch 96/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6255 - val_loss: 0.6167\n",
            "Epoch 97/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6255 - val_loss: 0.6167\n",
            "Epoch 98/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6255 - val_loss: 0.6167\n",
            "Epoch 99/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6255 - val_loss: 0.6167\n",
            "Epoch 100/100\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6255 - val_loss: 0.6167\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x15c5ae250>"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n",
        "\n",
        "#structure of the model\n",
        "\n",
        "model = kb.Sequential([\n",
        "    kb.layers.Dense(16, input_shape=[7], activation='sigmoid'),\n",
        "    kb.layers.Dense(8, activation='sigmoid'),\n",
        "    kb.layers.Dense(4, activation='sigmoid'),\n",
        "    kb.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "#how to train the model\n",
        "\n",
        "model.compile(loss = \"binary_crossentropy\",\n",
        "    optimizer = kb.optimizers.SGD())\n",
        "\n",
        "#fit the model (same as SKlearn)\n",
        "model.fit(X_train, y_train, epochs = 100, validation_data = (X_test, y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3MWeQAQxIFz"
      },
      "source": [
        "- Use `.predict()` to grab the train/test set predictions from your model\n",
        "- grab the ROC/AUC value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "96WKkOfVxIFz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.68168384]\n",
            " [0.68062186]\n",
            " [0.6819038 ]\n",
            " ...\n",
            " [0.6852464 ]\n",
            " [0.6873199 ]\n",
            " [0.68090796]]\n",
            "Train AUC:  0.56442999267841\n",
            "Test AUC:  0.547266904170715\n"
          ]
        }
      ],
      "source": [
        "# grab predictions\n",
        "\n",
        "y_pred_train = model.predict(X_train)\n",
        "y_pred_test = model.predict(X_test)\n",
        "\n",
        "# predict probabilities on train set\n",
        "\n",
        "print(y_pred_train)\n",
        "\n",
        "# assess performance\n",
        "\n",
        "print(\"Train AUC: \", roc_auc_score(y_train, y_pred_train))\n",
        "print(\"Test AUC: \", roc_auc_score(y_test, y_pred_test))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.68240446]\n",
            " [0.682346  ]\n",
            " [0.67949116]\n",
            " ...\n",
            " [0.6790647 ]\n",
            " [0.6782267 ]\n",
            " [0.6815529 ]]\n",
            "[[0.6809473 ]\n",
            " [0.6819583 ]\n",
            " [0.68253535]\n",
            " [0.67950916]\n",
            " [0.6829001 ]\n",
            " [0.6798272 ]\n",
            " [0.680624  ]\n",
            " [0.6764436 ]\n",
            " [0.6785183 ]\n",
            " [0.6811077 ]\n",
            " [0.6826779 ]\n",
            " [0.6831749 ]\n",
            " [0.6833189 ]\n",
            " [0.67897457]\n",
            " [0.6833773 ]\n",
            " [0.67509484]\n",
            " [0.6822651 ]\n",
            " [0.68265986]\n",
            " [0.68148965]\n",
            " [0.6840098 ]\n",
            " [0.6815848 ]\n",
            " [0.6830555 ]\n",
            " [0.6746122 ]\n",
            " [0.6767001 ]\n",
            " [0.6828811 ]\n",
            " [0.67451906]\n",
            " [0.68112755]\n",
            " [0.6830908 ]\n",
            " [0.68423545]\n",
            " [0.67526966]\n",
            " [0.68295646]\n",
            " [0.68391424]\n",
            " [0.68301535]\n",
            " [0.6830708 ]\n",
            " [0.68324167]\n",
            " [0.67694855]\n",
            " [0.6835274 ]\n",
            " [0.68118715]\n",
            " [0.68224716]\n",
            " [0.68321896]\n",
            " [0.6825776 ]\n",
            " [0.6795639 ]\n",
            " [0.6841161 ]\n",
            " [0.6839787 ]\n",
            " [0.68109417]\n",
            " [0.68295205]\n",
            " [0.6802069 ]\n",
            " [0.68248963]\n",
            " [0.6792517 ]\n",
            " [0.68248546]\n",
            " [0.6828546 ]\n",
            " [0.68088233]\n",
            " [0.68325424]\n",
            " [0.6759792 ]\n",
            " [0.6837053 ]\n",
            " [0.6841244 ]\n",
            " [0.6806557 ]\n",
            " [0.68288094]\n",
            " [0.68189764]\n",
            " [0.6831915 ]\n",
            " [0.680529  ]\n",
            " [0.6793625 ]\n",
            " [0.6817854 ]\n",
            " [0.681037  ]\n",
            " [0.6794043 ]\n",
            " [0.6846532 ]\n",
            " [0.68153507]\n",
            " [0.68260396]\n",
            " [0.68429947]\n",
            " [0.68106586]\n",
            " [0.6842613 ]\n",
            " [0.680108  ]\n",
            " [0.68311423]\n",
            " [0.68222874]\n",
            " [0.68265986]\n",
            " [0.68043834]\n",
            " [0.6836204 ]\n",
            " [0.68184835]\n",
            " [0.68137914]\n",
            " [0.6831262 ]\n",
            " [0.6828161 ]\n",
            " [0.68411183]\n",
            " [0.6828722 ]\n",
            " [0.6846712 ]\n",
            " [0.67964906]\n",
            " [0.6848246 ]\n",
            " [0.67866033]\n",
            " [0.6797588 ]\n",
            " [0.6837094 ]\n",
            " [0.6803746 ]\n",
            " [0.6778075 ]\n",
            " [0.6812594 ]\n",
            " [0.6792049 ]\n",
            " [0.68264794]\n",
            " [0.683681  ]\n",
            " [0.680624  ]\n",
            " [0.68268806]\n",
            " [0.68263143]\n",
            " [0.6811499 ]\n",
            " [0.677366  ]\n",
            " [0.6785004 ]\n",
            " [0.6807516 ]\n",
            " [0.6840464 ]\n",
            " [0.68330085]\n",
            " [0.67979926]\n",
            " [0.6726278 ]\n",
            " [0.68384665]\n",
            " [0.68337834]\n",
            " [0.6794945 ]\n",
            " [0.6840971 ]\n",
            " [0.6794669 ]\n",
            " [0.6823911 ]\n",
            " [0.6772023 ]\n",
            " [0.6794268 ]\n",
            " [0.683233  ]\n",
            " [0.6836027 ]\n",
            " [0.6821313 ]\n",
            " [0.6824095 ]\n",
            " [0.6792131 ]\n",
            " [0.6827869 ]\n",
            " [0.67655843]\n",
            " [0.68196636]\n",
            " [0.6824639 ]\n",
            " [0.67349035]\n",
            " [0.68269503]\n",
            " [0.6818148 ]\n",
            " [0.6846837 ]\n",
            " [0.6822888 ]\n",
            " [0.6817395 ]\n",
            " [0.67908686]\n",
            " [0.681529  ]\n",
            " [0.68003833]\n",
            " [0.68340975]\n",
            " [0.683941  ]\n",
            " [0.67704594]\n",
            " [0.67442775]\n",
            " [0.67793274]\n",
            " [0.68190926]\n",
            " [0.6844419 ]\n",
            " [0.6815701 ]\n",
            " [0.67919767]\n",
            " [0.6777801 ]\n",
            " [0.6845408 ]\n",
            " [0.6782731 ]\n",
            " [0.6808076 ]\n",
            " [0.68271035]\n",
            " [0.6782117 ]\n",
            " [0.68239737]\n",
            " [0.68411946]\n",
            " [0.679904  ]\n",
            " [0.6818471 ]\n",
            " [0.68237066]\n",
            " [0.68372566]\n",
            " [0.67840123]\n",
            " [0.6816434 ]\n",
            " [0.68091536]\n",
            " [0.67411685]\n",
            " [0.6816777 ]\n",
            " [0.6829388 ]\n",
            " [0.6821343 ]\n",
            " [0.6819816 ]\n",
            " [0.6832451 ]\n",
            " [0.68176824]\n",
            " [0.6789547 ]\n",
            " [0.6811499 ]\n",
            " [0.6833209 ]\n",
            " [0.683311  ]\n",
            " [0.67821616]\n",
            " [0.68248856]\n",
            " [0.68015504]\n",
            " [0.68153507]\n",
            " [0.6805841 ]\n",
            " [0.67960423]\n",
            " [0.6809356 ]\n",
            " [0.68464565]\n",
            " [0.6831336 ]\n",
            " [0.6829956 ]\n",
            " [0.6843541 ]\n",
            " [0.6800988 ]\n",
            " [0.6782292 ]\n",
            " [0.68008685]\n",
            " [0.68245167]\n",
            " [0.6820541 ]\n",
            " [0.67954284]\n",
            " [0.6842989 ]\n",
            " [0.67705977]\n",
            " [0.68322295]\n",
            " [0.683369  ]\n",
            " [0.6829233 ]\n",
            " [0.6782784 ]\n",
            " [0.68347025]\n",
            " [0.6830434 ]\n",
            " [0.6799871 ]\n",
            " [0.68196636]\n",
            " [0.6781654 ]\n",
            " [0.6825799 ]\n",
            " [0.6842609 ]\n",
            " [0.6818756 ]\n",
            " [0.6796009 ]\n",
            " [0.68423545]\n",
            " [0.68257624]\n",
            " [0.6739252 ]\n",
            " [0.67980355]\n",
            " [0.6752616 ]\n",
            " [0.6830254 ]\n",
            " [0.6813073 ]\n",
            " [0.6809223 ]\n",
            " [0.6820747 ]\n",
            " [0.6831209 ]\n",
            " [0.6832825 ]\n",
            " [0.6834744 ]\n",
            " [0.67974937]\n",
            " [0.6840582 ]\n",
            " [0.68248206]\n",
            " [0.6834368 ]\n",
            " [0.67914313]\n",
            " [0.6788677 ]\n",
            " [0.68354833]\n",
            " [0.6746141 ]\n",
            " [0.6822616 ]\n",
            " [0.6843225 ]\n",
            " [0.67834514]\n",
            " [0.68190926]\n",
            " [0.6791844 ]\n",
            " [0.68375   ]\n",
            " [0.683167  ]\n",
            " [0.68099284]\n",
            " [0.682784  ]\n",
            " [0.683744  ]\n",
            " [0.6777965 ]\n",
            " [0.6822628 ]\n",
            " [0.68232244]\n",
            " [0.68379974]\n",
            " [0.6689492 ]\n",
            " [0.68332577]\n",
            " [0.67826617]\n",
            " [0.68390477]\n",
            " [0.67862356]\n",
            " [0.6796    ]\n",
            " [0.6757665 ]\n",
            " [0.6757867 ]\n",
            " [0.6805793 ]\n",
            " [0.68303275]\n",
            " [0.6833441 ]\n",
            " [0.6758042 ]\n",
            " [0.6827939 ]\n",
            " [0.6792817 ]\n",
            " [0.67859954]\n",
            " [0.6827869 ]\n",
            " [0.6840373 ]\n",
            " [0.6837138 ]\n",
            " [0.6790707 ]\n",
            " [0.68291444]\n",
            " [0.68168426]\n",
            " [0.68067926]\n",
            " [0.6820253 ]\n",
            " [0.68319243]\n",
            " [0.6837535 ]\n",
            " [0.68364745]\n",
            " [0.6824894 ]\n",
            " [0.6835496 ]\n",
            " [0.6776924 ]\n",
            " [0.68268484]\n",
            " [0.68284106]\n",
            " [0.6785943 ]\n",
            " [0.6820019 ]\n",
            " [0.6786748 ]\n",
            " [0.6831211 ]\n",
            " [0.6834726 ]\n",
            " [0.6849446 ]\n",
            " [0.6776129 ]\n",
            " [0.680471  ]\n",
            " [0.68319345]\n",
            " [0.6828589 ]\n",
            " [0.6781107 ]\n",
            " [0.6785438 ]\n",
            " [0.6841944 ]\n",
            " [0.67655843]\n",
            " [0.6824651 ]\n",
            " [0.6821653 ]\n",
            " [0.678326  ]\n",
            " [0.6834966 ]\n",
            " [0.68391675]\n",
            " [0.682205  ]\n",
            " [0.6787357 ]\n",
            " [0.6827439 ]\n",
            " [0.6818361 ]\n",
            " [0.682491  ]\n",
            " [0.6790188 ]\n",
            " [0.6820682 ]\n",
            " [0.67752886]\n",
            " [0.6811413 ]\n",
            " [0.68303806]\n",
            " [0.67339873]\n",
            " [0.6805313 ]\n",
            " [0.6838674 ]\n",
            " [0.68239343]\n",
            " [0.67964494]\n",
            " [0.6812953 ]\n",
            " [0.6723813 ]\n",
            " [0.6802808 ]\n",
            " [0.6834966 ]\n",
            " [0.6833319 ]\n",
            " [0.6833013 ]\n",
            " [0.67736137]\n",
            " [0.68204385]\n",
            " [0.67840034]\n",
            " [0.6829956 ]\n",
            " [0.6810158 ]\n",
            " [0.6799703 ]\n",
            " [0.6811618 ]\n",
            " [0.6825305 ]\n",
            " [0.68186414]\n",
            " [0.6792292 ]\n",
            " [0.6838065 ]\n",
            " [0.68250906]\n",
            " [0.6800444 ]\n",
            " [0.68246835]\n",
            " [0.6798575 ]\n",
            " [0.67453706]\n",
            " [0.6788129 ]\n",
            " [0.6812961 ]\n",
            " [0.6824417 ]\n",
            " [0.68089175]\n",
            " [0.6823623 ]\n",
            " [0.68318164]\n",
            " [0.67967457]\n",
            " [0.6793058 ]\n",
            " [0.682838  ]\n",
            " [0.6835145 ]\n",
            " [0.6790815 ]\n",
            " [0.6740687 ]\n",
            " [0.68301034]\n",
            " [0.6784665 ]\n",
            " [0.6741322 ]\n",
            " [0.68421674]\n",
            " [0.6806195 ]\n",
            " [0.684121  ]\n",
            " [0.68437123]\n",
            " [0.68311423]\n",
            " [0.6816437 ]\n",
            " [0.6743392 ]\n",
            " [0.67725396]\n",
            " [0.68205225]\n",
            " [0.6781999 ]\n",
            " [0.6799315 ]\n",
            " [0.6804824 ]\n",
            " [0.68333   ]\n",
            " [0.6810594 ]\n",
            " [0.67970073]\n",
            " [0.68237716]\n",
            " [0.6813692 ]\n",
            " [0.67947817]\n",
            " [0.68184143]\n",
            " [0.6839385 ]\n",
            " [0.68239343]\n",
            " [0.683763  ]\n",
            " [0.6846532 ]\n",
            " [0.68324167]\n",
            " [0.68219966]\n",
            " [0.68079567]\n",
            " [0.68425846]\n",
            " [0.6811594 ]\n",
            " [0.6839471 ]\n",
            " [0.68238163]\n",
            " [0.68231285]\n",
            " [0.6805805 ]\n",
            " [0.67973214]\n",
            " [0.6805723 ]\n",
            " [0.6828722 ]\n",
            " [0.6829054 ]\n",
            " [0.68247676]\n",
            " [0.68326175]\n",
            " [0.67993677]\n",
            " [0.68087804]\n",
            " [0.6825417 ]\n",
            " [0.68038535]\n",
            " [0.68250906]\n",
            " [0.6799871 ]\n",
            " [0.6833746 ]\n",
            " [0.68382895]\n",
            " [0.67836857]\n",
            " [0.68025374]\n",
            " [0.6808318 ]\n",
            " [0.6818166 ]\n",
            " [0.6796713 ]\n",
            " [0.68423903]\n",
            " [0.67360365]\n",
            " [0.6816299 ]\n",
            " [0.68337643]\n",
            " [0.68436986]\n",
            " [0.68182206]\n",
            " [0.68413126]\n",
            " [0.6822152 ]\n",
            " [0.6816049 ]\n",
            " [0.68362594]\n",
            " [0.68375564]\n",
            " [0.6785041 ]\n",
            " [0.6840675 ]\n",
            " [0.68294984]\n",
            " [0.68008095]\n",
            " [0.678895  ]\n",
            " [0.68138576]\n",
            " [0.68145597]\n",
            " [0.6774244 ]\n",
            " [0.68241   ]\n",
            " [0.6833787 ]\n",
            " [0.6774576 ]\n",
            " [0.6831429 ]\n",
            " [0.6808141 ]\n",
            " [0.68279433]\n",
            " [0.6800695 ]\n",
            " [0.683763  ]\n",
            " [0.6773881 ]\n",
            " [0.68397456]\n",
            " [0.6830293 ]\n",
            " [0.6764986 ]\n",
            " [0.6753335 ]\n",
            " [0.6843725 ]\n",
            " [0.6761956 ]\n",
            " [0.6792308 ]\n",
            " [0.6811618 ]\n",
            " [0.682784  ]\n",
            " [0.6785169 ]\n",
            " [0.68239987]\n",
            " [0.6779167 ]\n",
            " [0.684121  ]\n",
            " [0.6826502 ]\n",
            " [0.68208635]\n",
            " [0.6775843 ]\n",
            " [0.6809295 ]\n",
            " [0.6822006 ]\n",
            " [0.6842318 ]\n",
            " [0.68367183]\n",
            " [0.67975575]\n",
            " [0.682175  ]\n",
            " [0.68113697]\n",
            " [0.6768395 ]\n",
            " [0.6840083 ]\n",
            " [0.6815296 ]\n",
            " [0.6820876 ]\n",
            " [0.67973787]\n",
            " [0.67998755]\n",
            " [0.6822394 ]\n",
            " [0.68231285]\n",
            " [0.67320716]\n",
            " [0.6829057 ]\n",
            " [0.6804515 ]\n",
            " [0.6830611 ]\n",
            " [0.6824616 ]\n",
            " [0.68364346]\n",
            " [0.68227595]\n",
            " [0.6834715 ]\n",
            " [0.68087804]\n",
            " [0.6799531 ]\n",
            " [0.68273956]\n",
            " [0.6813177 ]\n",
            " [0.67380244]\n",
            " [0.6774106 ]\n",
            " [0.68398786]\n",
            " [0.68376434]\n",
            " [0.68231285]\n",
            " [0.6825776 ]\n",
            " [0.68237597]\n",
            " [0.68074036]\n",
            " [0.68330216]\n",
            " [0.67896366]\n",
            " [0.68437886]\n",
            " [0.6792308 ]\n",
            " [0.68105775]\n",
            " [0.68332475]\n",
            " [0.6786268 ]\n",
            " [0.680342  ]\n",
            " [0.6784669 ]\n",
            " [0.6824572 ]\n",
            " [0.6831702 ]\n",
            " [0.68384707]\n",
            " [0.6809689 ]\n",
            " [0.68409324]\n",
            " [0.6784003 ]\n",
            " [0.67985827]\n",
            " [0.67973447]\n",
            " [0.68401176]\n",
            " [0.67762214]\n",
            " [0.68316346]\n",
            " [0.68301535]\n",
            " [0.67978656]\n",
            " [0.6794339 ]\n",
            " [0.6785416 ]\n",
            " [0.68360066]\n",
            " [0.6833012 ]\n",
            " [0.6842989 ]\n",
            " [0.6835273 ]\n",
            " [0.6782206 ]\n",
            " [0.68033063]\n",
            " [0.67815983]\n",
            " [0.6748189 ]\n",
            " [0.6812694 ]\n",
            " [0.67925537]\n",
            " [0.68115103]\n",
            " [0.6831367 ]\n",
            " [0.6828514 ]\n",
            " [0.68205285]\n",
            " [0.6830822 ]\n",
            " [0.67861056]\n",
            " [0.6827428 ]\n",
            " [0.6842955 ]\n",
            " [0.68110436]\n",
            " [0.67807627]\n",
            " [0.67905277]\n",
            " [0.671682  ]]\n"
          ]
        }
      ],
      "source": [
        "# calculate ROC/AUC\n",
        "\n",
        "auc_train = roc_auc_score(y_train, train_preds)\n",
        "auc_test = roc_auc_score(y_test, test_preds)\n",
        "\n",
        "print(train_preds)\n",
        "print(test_preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EE2zOm1wxIF0"
      },
      "source": [
        "\n",
        "\n",
        "## Building a Feed Forward NN for Black and White Digit Images\n",
        "\n",
        "The famous dataset, MNIST, contains thousands of black and white pictures of handwritten digits. While other NN structures (like Convolutional NNs that we cover next) are built specifically to learn from images, when the images are simple and Black and White, even a Feed Forward NN can perform okay!\n",
        "\n",
        "So let's build a NN that classifies handwritten digits.\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1xZhgplIHkTkbKoL6SuWGHn_DL0AbNSPh\" alt=\"mnist data\" width = \"500px\" /></center>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "DBrToYhuxIF0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 2s 0us/step\n",
            "11501568/11490434 [==============================] - 2s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# load and rescale the data so that the pixel values are between 0-1\n",
        "\n",
        "# load data\n",
        "((trainX, trainY), (testX, testY)) = mnist.load_data()\n",
        "\n",
        "# reshape datta to be individual columns instead of image matrix\n",
        "trainX = trainX.reshape((trainX.shape[0], 28 * 28 * 1))\n",
        "testX = testX.reshape((testX.shape[0], 28 * 28 * 1))\n",
        "\n",
        "# rescale data to be 0-1 instead of 0-255\n",
        "trainX = trainX.astype(\"float32\") / 255.0\n",
        "testX = testX.astype(\"float32\") / 255.0\n",
        "\n",
        "# change the labels to be in the correct format\n",
        "lb = LabelBinarizer()\n",
        "trainY = lb.fit_transform(trainY)\n",
        "testY = lb.transform(testY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-jAUgYixIF0"
      },
      "source": [
        "- Build a Deep Neural Network with at least 5 `Dense()` layers (but you can have 7, 10, or more!)\n",
        "- the first layer needs an `input_shape` of `[784]` because there are 784 pixels per image (28x28)\n",
        "- the last layer needs to have 10 nodes (one for each digit)\n",
        "- the last layer needs to use a softmax activation so that each of our nodes represents the probability of being each digit\n",
        "- you can choose the # of nodes for each of the hidden layers (typically we reduce the number of nodes each layer but that's not required)\n",
        "- compile your model using `loss = \"categorical_crossentropy\"`, and either SGD or Adam as your optimizer\n",
        "- choose a reasonable learning rate or use default\n",
        "- during training, ask your model to print out the accuracy of the model\n",
        "- train model for 100+ epochs\n",
        "\n",
        "### Question\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" alt=\"Q\" width = \"200\"/>\n",
        "\n",
        "How does your model do? Are you surprised by the performance of the model given its structure isn't designed to handle images?\n",
        "\n",
        "### Question\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" alt=\"Q\" width = \"200\"/>\n",
        "\n",
        "We fed in a `784x1` array/vector of pixel values (i.e. we had 784 features) into our Feed Forward NN. Does this structure take into acount the spatial nature of an image (where pixels surrounding a pixel are likely related)? Do you think that type of structure would be helpful for processing more complicated images?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "urN_i-7XxIF0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1875/1875 [==============================] - 3s 1ms/step - loss: 2.3027\n",
            "Epoch 2/100\n",
            "1875/1875 [==============================] - 2s 1ms/step - loss: 2.3013\n",
            "Epoch 3/100\n",
            "1390/1875 [=====================>........] - ETA: 0s - loss: 2.3011"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/min-is/Chapman-University/CPSC 393 - Machine Learning/Classwork/5 - Neural Networks I.ipynb Cell 19'\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://github/min-is/Chapman-University/CPSC%20393%20-%20Machine%20Learning/Classwork/5%20-%20Neural%20Networks%20I.ipynb#ch0000017vscode-vfs?line=15'>16</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(loss \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://github/min-is/Chapman-University/CPSC%20393%20-%20Machine%20Learning/Classwork/5%20-%20Neural%20Networks%20I.ipynb#ch0000017vscode-vfs?line=16'>17</a>\u001b[0m     optimizer \u001b[39m=\u001b[39m kb\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mSGD())\n\u001b[1;32m     <a href='vscode-notebook-cell://github/min-is/Chapman-University/CPSC%20393%20-%20Machine%20Learning/Classwork/5%20-%20Neural%20Networks%20I.ipynb#ch0000017vscode-vfs?line=18'>19</a>\u001b[0m \u001b[39m#fit the model (same as SKlearn)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://github/min-is/Chapman-University/CPSC%20393%20-%20Machine%20Learning/Classwork/5%20-%20Neural%20Networks%20I.ipynb#ch0000017vscode-vfs?line=19'>20</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(trainX, trainY, epochs \u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://github/min-is/Chapman-University/CPSC%20393%20-%20Machine%20Learning/Classwork/5%20-%20Neural%20Networks%20I.ipynb#ch0000017vscode-vfs?line=21'>22</a>\u001b[0m \u001b[39m# compile model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://github/min-is/Chapman-University/CPSC%20393%20-%20Machine%20Learning/Classwork/5%20-%20Neural%20Networks%20I.ipynb#ch0000017vscode-vfs?line=22'>23</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://github/min-is/Chapman-University/CPSC%20393%20-%20Machine%20Learning/Classwork/5%20-%20Neural%20Networks%20I.ipynb#ch0000017vscode-vfs?line=23'>24</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://github/min-is/Chapman-University/CPSC%20393%20-%20Machine%20Learning/Classwork/5%20-%20Neural%20Networks%20I.ipynb#ch0000017vscode-vfs?line=24'>25</a>\u001b[0m \u001b[39m#fit the model (same as SKlearn)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://github/min-is/Chapman-University/CPSC%20393%20-%20Machine%20Learning/Classwork/5%20-%20Neural%20Networks%20I.ipynb#ch0000017vscode-vfs?line=25'>26</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://github/min-is/Chapman-University/CPSC%20393%20-%20Machine%20Learning/Classwork/5%20-%20Neural%20Networks%20I.ipynb#ch0000017vscode-vfs?line=26'>27</a>\u001b[0m \u001b[39m# predict probabilities on train set\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://github/min-is/Chapman-University/CPSC%20393%20-%20Machine%20Learning/Classwork/5%20-%20Neural%20Networks%20I.ipynb#ch0000017vscode-vfs?line=28'>29</a>\u001b[0m \u001b[39mprint\u001b[39m(y_pred_train)\n",
            "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=61'>62</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=62'>63</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///usr/local/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=63'>64</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/keras/engine/training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/keras/engine/training.py?line=1376'>1377</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/keras/engine/training.py?line=1377'>1378</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/keras/engine/training.py?line=1378'>1379</a>\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/keras/engine/training.py?line=1379'>1380</a>\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/keras/engine/training.py?line=1380'>1381</a>\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/keras/engine/training.py?line=1381'>1382</a>\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/keras/engine/training.py?line=1382'>1383</a>\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> <a href='file:///usr/local/lib/python3.8/site-packages/keras/engine/training.py?line=1383'>1384</a>\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/keras/engine/training.py?line=1384'>1385</a>\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/keras/engine/training.py?line=1385'>1386</a>\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
            "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=147'>148</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=148'>149</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=149'>150</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=150'>151</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=151'>152</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=911'>912</a>\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=913'>914</a>\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=914'>915</a>\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=916'>917</a>\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=917'>918</a>\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
            "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=943'>944</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=944'>945</a>\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=945'>946</a>\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=946'>947</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=947'>948</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=948'>949</a>\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=949'>950</a>\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=950'>951</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
            "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=2952'>2953</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=2953'>2954</a>\u001b[0m   (graph_function,\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=2954'>2955</a>\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=2955'>2956</a>\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=2956'>2957</a>\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
            "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1848'>1849</a>\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1849'>1850</a>\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1850'>1851</a>\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1851'>1852</a>\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1852'>1853</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1853'>1854</a>\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1854'>1855</a>\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1855'>1856</a>\u001b[0m     args,\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1856'>1857</a>\u001b[0m     possible_gradient_type,\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1857'>1858</a>\u001b[0m     executing_eagerly)\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1858'>1859</a>\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
            "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=496'>497</a>\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=497'>498</a>\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=498'>499</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=499'>500</a>\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=500'>501</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=501'>502</a>\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=502'>503</a>\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=503'>504</a>\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=504'>505</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=505'>506</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=506'>507</a>\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=507'>508</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=510'>511</a>\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=511'>512</a>\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
            "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py?line=51'>52</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py?line=52'>53</a>\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py?line=53'>54</a>\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py?line=54'>55</a>\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py?line=55'>56</a>\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py?line=56'>57</a>\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# build structure of the model\n",
        "\n",
        "model = kb.Sequential([\n",
        "    kb.layers.Dense(18, input_shape=[784], activation='softmax'),\n",
        "    kb.layers.Dense(17, activation='softmax'),\n",
        "    kb.layers.Dense(16, activation='softmax'),\n",
        "    kb.layers.Dense(15, activation='softmax'),\n",
        "    kb.layers.Dense(14, activation = 'softmax'),\n",
        "    kb.layers.Dense(13, activation = 'softmax'),\n",
        "    kb.layers.Dense(10, activation = 'softmax'),\n",
        "\n",
        "])\n",
        "\n",
        "#how to train the model\n",
        "\n",
        "model.compile(loss = \"categorical_crossentropy\",\n",
        "    optimizer = kb.optimizers.SGD())\n",
        "\n",
        "#fit the model (same as SKlearn)\n",
        "model.fit(trainX, trainY, epochs = 100)\n",
        "\n",
        "# compile model\n",
        "\n",
        "\n",
        "#fit the model (same as SKlearn)\n",
        "\n",
        "# predict probabilities on train set\n",
        "\n",
        "print(y_pred_train)\n",
        "\n",
        "# assess performance\n",
        "\n",
        "print(\"Train AUC: \", roc_auc_score(y_train, y_pred_train))\n",
        "print(\"Test AUC: \", roc_auc_score(y_test, y_pred_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7x_3kQWxIF1"
      },
      "source": [
        "- Grab the weights from your model using `get_weights()`.\n",
        "\n",
        "\n",
        "### Question\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" alt=\"Q\" width = \"200\"/>\n",
        "\n",
        "\n",
        "How many weights are there? Are these weights *interpretable* (e.g. with coefficients in a linear regression, the coefficients are interpretable...they easily tell us the relationship between our inputs and our outputs)? If I wanted to know the exact effect of the first feature/pixel on the predicted outcome, would you be able to grab that easily? Does the word black box seem appropriate?\n",
        "\n",
        "\n",
        "### Question\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" alt=\"Q\" width = \"200\"/>\n",
        "\n",
        "\n",
        "Using this image, identify which nodes' values are affected by the first pixel value (highlighted)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr040x5kxIF1"
      },
      "source": [
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1XQh1nvMax1Hc5r9ITz-RiJDasyT5-V2b\" alt=\"Q\" width = \"700\"/></center>\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
