{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OZT6wL4xIFs",
        "outputId": "6300fcfd-36d8-4f1f-c289-0895a24b9650"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from keras.datasets import mnist\n",
        "import tensorflow.keras as kb\n",
        "from tensorflow.keras import backend\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "\n",
        "from plotnine import *\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, roc_auc_score\n",
        "\n",
        "from sklearn.linear_model import LinearRegression # Linear Regression Model\n",
        "from sklearn.preprocessing import StandardScaler #Z-score variables\n",
        "\n",
        "from sklearn.model_selection import train_test_split # simple TT split cv\n",
        "from sklearn.model_selection import KFold # k-fold cv\n",
        "from sklearn.model_selection import LeaveOneOut #LOO cv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hPEQ6HJxIFu"
      },
      "source": [
        "# Together\n",
        "Neural Networks are more of a framework for building custom models than a specific type of model. When building a neural network you need to think about 3 main things:\n",
        "\n",
        "1. The structure of your model (how many layers? how many nodes? what activation functions?)\n",
        "2. The loss function for your model\n",
        "3. The optimizer you'll use to train (we'll get to that in the next NN lecture)\n",
        "\n",
        "Your input and output layers are pre-determined by the number and type of input you want to go into your model, and the form of output that you want. For example for a regression problem you likely want a single node in the output layer. For a multi-class classification problem, you want `n` nodes in the output layer, one for each possible output category. \n",
        "\n",
        "\n",
        "## Review\n",
        "\n",
        "### Layers, Hidden Layers, Nodes\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1iN4PCbZyIDQCgKx9odEpTaXpHOc-vZzg\" alt=\"Q\" width = \"400\"/>\n",
        "\n",
        "### Calculating the Value of a Node\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1P_jfOCXs4wFUVpyFeRH3ysAIaeoHSe54\" alt=\"Q\" width = \"400\"/>\n",
        "\n",
        "### Activation Functions\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1CXSZZ9NjMlvdtopwGD5mAlFMukAmPBrQ\" alt=\"Q\" width = \"400\"/>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1I9vs6IgnHlohBkTt0G7E-vHLBQTT169_\" alt=\"Q\" width = \"400\"/>\n",
        "\n",
        "Softmax:\n",
        "$$ f(x) = \\frac{e^{z_i}}{\\sum_{j=1}^N e^z_j}$$\n",
        "\n",
        "The softmax activation creates a vector of probabilities that sum to 1, and as a bonus, it's ✨differentiable✨.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_JHdD7-xIFw",
        "outputId": "9cff0fe9-e06a-48d0-da70-15d5c9f0d08b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([0.001, 0.089, 0.243, 0.661, 0.004, 0.002])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = np.array([1,6,7,8,3,2])\n",
        "soft_x = np.exp(x)\n",
        "soft_x = soft_x/np.sum(soft_x)\n",
        "print(np.sum(soft_x))\n",
        "np.round(soft_x,3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W07zJ8WMxIFw"
      },
      "source": [
        "\n",
        "\n",
        "## Building a Simple NN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pPxv2xKxIFx"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/Music_data.csv\")\n",
        "feats = [\"danceability\", \"energy\", \"loudness\",\"acousticness\", \"instrumentalness\", \"liveness\", \"duration_ms\"]\n",
        "predict = \"valence\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09gw1YGGxIFx"
      },
      "outputs": [],
      "source": [
        "# Regression\n",
        "\n",
        "#structure of the model\n",
        "\n",
        "\n",
        "#how to train the model\n",
        "\n",
        "#fit the model (same as SKlearn)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SotJVPNKxIFx"
      },
      "outputs": [],
      "source": [
        "# get weights from Neural Network\n",
        "model.get_weights()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeKL9A0kxIFy"
      },
      "source": [
        "# In Class\n",
        "\n",
        "## Building a Logistic Regression NN\n",
        "Let's change the model we built together so that it predicts `mode` instead of valence. Because we're now predicting a categorical variable (or rather, we're predicting the probability that the data is in a given category), we need to change the last layer of our network. Copy and paste the model we made together, and let's make some changes. \n",
        "\n",
        "- Change your `y` to be `mode` \n",
        "- Add a `activation = \"sigmoid\"` argument to your Layer\n",
        "- Change the `loss` argument in your `model.compile()` function to be `\"binary_crossentropy\"` (remember binary cross entropy is another name for log loss which we learned about in 392)\n",
        "\n",
        "\n",
        "Then fit your model and see how well it does!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZ9m7uDFxIFy"
      },
      "outputs": [],
      "source": [
        "# change y and redo TTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PvcYGvpxIFy"
      },
      "outputs": [],
      "source": [
        "#structure of the model\n",
        "\n",
        "\n",
        "#how to train the model\n",
        "\n",
        "\n",
        "#fit the model (same as SKlearn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Nx_ZW3sxIFy"
      },
      "source": [
        "## Making that Model DEEP\n",
        "\n",
        "Now, let's modify your model once more, this time to add hidden layers.\n",
        "\n",
        "- Your first `Dense()` layer should still have an `input_shape` of `[7]` but now you can make that hidden layer ANY SIZE you want\n",
        "- add at LEAST 3 more `Dense()` layers with however many nodes you want (play around with it)\n",
        "- make sure your last `Dense()` layer has an output dimension of `1` and a `sigmoid` activation\n",
        "- compile and fit your model\n",
        "\n",
        "### Question\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" alt=\"Q\" width = \"200\"/>\n",
        "\n",
        "How does the performance of your model change from the very simple NN you built before? Is there any overfitting going on?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjQXfDypxIFz"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n",
        "#structure of the model\n",
        "\n",
        "\n",
        "#how to train the model\n",
        "\n",
        "\n",
        "#fit the model (same as SKlearn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3MWeQAQxIFz"
      },
      "source": [
        "- Use `.predict()` to grab the train/test set predictions from your model\n",
        "- grab the ROC/AUC value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96WKkOfVxIFz"
      },
      "outputs": [],
      "source": [
        "# grab predictions\n",
        "\n",
        "# calculate ROC/AUC\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EE2zOm1wxIF0"
      },
      "source": [
        "\n",
        "\n",
        "## Building a Feed Forward NN for Black and White Digit Images\n",
        "\n",
        "The famous dataset, MNIST, contains thousands of black and white pictures of handwritten digits. While other NN structures (like Convolutional NNs that we cover next) are built specifically to learn from images, when the images are simple and Black and White, even a Feed Forward NN can perform okay!\n",
        "\n",
        "So let's build a NN that classifies handwritten digits.\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1xZhgplIHkTkbKoL6SuWGHn_DL0AbNSPh\" alt=\"mnist data\" width = \"500px\" /></center>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBrToYhuxIF0"
      },
      "outputs": [],
      "source": [
        "# load and rescale the data so that the pixel values are between 0-1\n",
        "\n",
        "# load data\n",
        "((trainX, trainY), (testX, testY)) = mnist.load_data()\n",
        "\n",
        "# reshape datta to be individual columns instead of image matrix\n",
        "trainX = trainX.reshape((trainX.shape[0], 28 * 28 * 1))\n",
        "testX = testX.reshape((testX.shape[0], 28 * 28 * 1))\n",
        "\n",
        "# rescale data to be 0-1 instead of 0-255\n",
        "trainX = trainX.astype(\"float32\") / 255.0\n",
        "testX = testX.astype(\"float32\") / 255.0\n",
        "\n",
        "# change the labels to be in the correct format\n",
        "lb = LabelBinarizer()\n",
        "trainY = lb.fit_transform(trainY)\n",
        "testY = lb.transform(testY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-jAUgYixIF0"
      },
      "source": [
        "- Build a Deep Neural Network with at least 5 `Dense()` layers (but you can have 7, 10, or more!)\n",
        "- the first layer needs an `input_shape` of `[784]` because there are 784 pixels per image (28x28)\n",
        "- the last layer needs to have 10 nodes (one for each digit)\n",
        "- the last layer needs to use a softmax activation so that each of our nodes represents the probability of being each digit\n",
        "- you can choose the # of nodes for each of the hidden layers (typically we reduce the number of nodes each layer but that's not required)\n",
        "- compile your model using `loss = \"categorical_crossentropy\"`, and either SGD or Adam as your optimizer\n",
        "- choose a reasonable learning rate or use default\n",
        "- during training, ask your model to print out the accuracy of the model\n",
        "- train model for 100+ epochs\n",
        "\n",
        "### Question\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" alt=\"Q\" width = \"200\"/>\n",
        "\n",
        "How does your model do? Are you surprised by the performance of the model given its structure isn't designed to handle images?\n",
        "\n",
        "### Question\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" alt=\"Q\" width = \"200\"/>\n",
        "\n",
        "We fed in a `784x1` array/vector of pixel values (i.e. we had 784 features) into our Feed Forward NN. Does this structure take into acount the spatial nature of an image (where pixels surrounding a pixel are likely related)? Do you think that type of structure would be helpful for processing more complicated images?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urN_i-7XxIF0"
      },
      "outputs": [],
      "source": [
        "# build structure of the model\n",
        "\n",
        "\n",
        "# compile model\n",
        "\n",
        "\n",
        "#fit the model (same as SKlearn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7x_3kQWxIF1"
      },
      "source": [
        "- Grab the weights from your model using `get_weights()`.\n",
        "\n",
        "\n",
        "### Question\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" alt=\"Q\" width = \"200\"/>\n",
        "\n",
        "\n",
        "How many weights are there? Are these weights *interpretable* (e.g. with coefficients in a linear regression, the coefficients are interpretable...they easily tell us the relationship between our inputs and our outputs)? If I wanted to know the exact effect of the first feature/pixel on the predicted outcome, would you be able to grab that easily? Does the word black box seem appropriate?\n",
        "\n",
        "\n",
        "### Question\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" alt=\"Q\" width = \"200\"/>\n",
        "\n",
        "\n",
        "Using this image, identify which nodes' values are affected by the first pixel value (highlighted)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr040x5kxIF1"
      },
      "source": [
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1XQh1nvMax1Hc5r9ITz-RiJDasyT5-V2b\" alt=\"Q\" width = \"700\"/></center>\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}