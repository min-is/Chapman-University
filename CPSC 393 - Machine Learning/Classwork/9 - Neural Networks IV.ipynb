{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rKGXxQMj3PkI"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from keras.datasets import mnist\n",
        "import tensorflow.keras as kb\n",
        "from tensorflow.keras import backend\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "\n",
        "from plotnine import *\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, roc_auc_score\n",
        "\n",
        "from sklearn.linear_model import LinearRegression # Linear Regression Model\n",
        "from sklearn.preprocessing import StandardScaler #Z-score variables\n",
        "\n",
        "from sklearn.model_selection import train_test_split # simple TT split cv\n",
        "from sklearn.model_selection import KFold # k-fold cv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmYtumHU3PkJ"
      },
      "source": [
        "# Regularization in NNs\n",
        "\n",
        "\n",
        "In CPSC 392, we talked about Regularization through things like LASSO/Ridge penalties, Bagging in Random Forests, and Limiting `max_depth` or `min_samples_leaf` in Decision Trees. All of these methods aim to make our models *less complex* which can move us from an area of high variance/overfitting, to an area where we are NOT overfitting nor underfitting (our main goal.)\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1akEMD7_STAkqJcZUI5vc3CW5aU5AeTWR\"></center>\n",
        "\n",
        "However, the models we learned in 392 tended to be pretty simple. Deep Neural Networks are typically *NOT* simple. This complexity allows them to learn complicated relationships between our inputs and our outputs. For example, with linear regression, you're limited to linear relationships between inputs and outputs, neural networks allow you to have non-linear relationships, and the deeper the network the more complicated those relationships can be (remember we talked about Universal Function Approximation?)\n",
        "\n",
        "Remember the whole *goal* of Machine Learning is to approximate the function $y = f(x)$ that tells you how to take our inputs $x$ and turn them into an accurate guess at our output(s) $y$. That function is $f(x)$. The more complicated $f(x)$ is, the more flexible/complex our model needs to be to approximate it. Models like regression, decision trees...etc. often aren't flexible enough to solve complicated problems like the one's we'll encounter later in the class. BUT Neural Networks can be!\n",
        "\n",
        "However, we know that when a model is TOO complex/flexible, it can overfit...\n",
        "\n",
        "<center><img src=\"https://jashrathod.github.io/assets/img/optimal-fit.png\" alt = \"from:https://jashrathod.github.io/2021-09-30-underfitting-overfitting-and-regularization/\" width = \"400px\" ></center>\n",
        "\n",
        "\n",
        "So we often need regularization to make sure that our models are not overfitting. Remember that **overfitting is when the model is so complex that it can pick up on noise/quirks in the training data that aren't there in other samples of data** so our goal when regularizing is to prevent this from happening. \n",
        "\n",
        "We talked about *multiple* methods of Regularization for NNs:\n",
        "\n",
        "- Penalization\n",
        "- Bagging or Model Averaging\n",
        "- Dropout\n",
        "- Early Stopping\n",
        "- Batch Normalization\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlOYIVYI3PkL"
      },
      "source": [
        "# Regularization in keras\n",
        "\n",
        "## Penalization\n",
        "\n",
        "We can add penalization to any/all our layers using the `kernel_regularizer` argument. While you can create your own custom regularizers, we'll focus on `L1`, `L2` and ElasticNet aka `L1L2`. Let's take the MNIST model we built in Classwork 5 and add penalties to the layers. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "53SOGM2v3PkM"
      },
      "outputs": [],
      "source": [
        "# load and rescale the data so that the pixel values are between 0-1\n",
        "\n",
        "# load data\n",
        "((trainX, trainY), (testX, testY)) = mnist.load_data()\n",
        "\n",
        "# reshape datta to be individual columns instead of image matrix\n",
        "trainX = trainX.reshape((trainX.shape[0], 28 * 28 * 1))\n",
        "testX = testX.reshape((testX.shape[0], 28 * 28 * 1))\n",
        "\n",
        "# rescale data to be 0-1 instead of 0-255\n",
        "trainX = trainX.astype(\"float32\") / 255.0\n",
        "testX = testX.astype(\"float32\") / 255.0\n",
        "\n",
        "# change the labels to be in the correct format\n",
        "lb = LabelBinarizer()\n",
        "trainY = lb.fit_transform(trainY)\n",
        "testY = lb.transform(testY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "50pfAHVk3PkM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-03-02 11:53:29.764553: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.5020 - accuracy: 0.8526 - val_loss: 0.3431 - val_accuracy: 0.9017\n",
            "Epoch 2/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3270 - accuracy: 0.9074 - val_loss: 0.3068 - val_accuracy: 0.9143\n",
            "Epoch 3/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3060 - accuracy: 0.9132 - val_loss: 0.2936 - val_accuracy: 0.9188\n",
            "Epoch 4/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2946 - accuracy: 0.9170 - val_loss: 0.3042 - val_accuracy: 0.9101\n",
            "Epoch 5/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2892 - accuracy: 0.9189 - val_loss: 0.3153 - val_accuracy: 0.9083\n",
            "Epoch 6/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2842 - accuracy: 0.9214 - val_loss: 0.2909 - val_accuracy: 0.9172\n",
            "Epoch 7/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2817 - accuracy: 0.9203 - val_loss: 0.2918 - val_accuracy: 0.9155\n",
            "Epoch 8/100\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2780 - accuracy: 0.9218 - val_loss: 0.2850 - val_accuracy: 0.9199\n",
            "Epoch 9/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2766 - accuracy: 0.9221 - val_loss: 0.2798 - val_accuracy: 0.9214\n",
            "Epoch 10/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2738 - accuracy: 0.9231 - val_loss: 0.2863 - val_accuracy: 0.9194\n",
            "Epoch 11/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2729 - accuracy: 0.9229 - val_loss: 0.2856 - val_accuracy: 0.9187\n",
            "Epoch 12/100\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2711 - accuracy: 0.9246 - val_loss: 0.2747 - val_accuracy: 0.9217\n",
            "Epoch 13/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2694 - accuracy: 0.9241 - val_loss: 0.2784 - val_accuracy: 0.9235\n",
            "Epoch 14/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2688 - accuracy: 0.9246 - val_loss: 0.2983 - val_accuracy: 0.9156\n",
            "Epoch 15/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2670 - accuracy: 0.9254 - val_loss: 0.2785 - val_accuracy: 0.9224\n",
            "Epoch 16/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2665 - accuracy: 0.9248 - val_loss: 0.2795 - val_accuracy: 0.9226\n",
            "Epoch 17/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2644 - accuracy: 0.9265 - val_loss: 0.2865 - val_accuracy: 0.9196\n",
            "Epoch 18/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2649 - accuracy: 0.9262 - val_loss: 0.2783 - val_accuracy: 0.9227\n",
            "Epoch 19/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2628 - accuracy: 0.9264 - val_loss: 0.2770 - val_accuracy: 0.9221\n",
            "Epoch 20/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2621 - accuracy: 0.9268 - val_loss: 0.2770 - val_accuracy: 0.9245\n",
            "Epoch 21/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2618 - accuracy: 0.9260 - val_loss: 0.2793 - val_accuracy: 0.9192\n",
            "Epoch 22/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2607 - accuracy: 0.9272 - val_loss: 0.2807 - val_accuracy: 0.9212\n",
            "Epoch 23/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2609 - accuracy: 0.9269 - val_loss: 0.2762 - val_accuracy: 0.9235\n",
            "Epoch 24/100\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2598 - accuracy: 0.9263 - val_loss: 0.2923 - val_accuracy: 0.9216\n",
            "Epoch 25/100\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2584 - accuracy: 0.9279 - val_loss: 0.2811 - val_accuracy: 0.9215\n",
            "Epoch 26/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2588 - accuracy: 0.9282 - val_loss: 0.2756 - val_accuracy: 0.9232\n",
            "Epoch 27/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2583 - accuracy: 0.9280 - val_loss: 0.2835 - val_accuracy: 0.9223\n",
            "Epoch 28/100\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2570 - accuracy: 0.9279 - val_loss: 0.2754 - val_accuracy: 0.9239\n",
            "Epoch 29/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2572 - accuracy: 0.9280 - val_loss: 0.2771 - val_accuracy: 0.9248\n",
            "Epoch 30/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2566 - accuracy: 0.9278 - val_loss: 0.2799 - val_accuracy: 0.9238\n",
            "Epoch 31/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2569 - accuracy: 0.9283 - val_loss: 0.2831 - val_accuracy: 0.9226\n",
            "Epoch 32/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2554 - accuracy: 0.9286 - val_loss: 0.2783 - val_accuracy: 0.9245\n",
            "Epoch 33/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2548 - accuracy: 0.9288 - val_loss: 0.2829 - val_accuracy: 0.9216\n",
            "Epoch 34/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2543 - accuracy: 0.9288 - val_loss: 0.2788 - val_accuracy: 0.9234\n",
            "Epoch 35/100\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2545 - accuracy: 0.9281 - val_loss: 0.2847 - val_accuracy: 0.9230\n",
            "Epoch 36/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2547 - accuracy: 0.9294 - val_loss: 0.2828 - val_accuracy: 0.9229\n",
            "Epoch 37/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2532 - accuracy: 0.9289 - val_loss: 0.2736 - val_accuracy: 0.9244\n",
            "Epoch 38/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2536 - accuracy: 0.9290 - val_loss: 0.2740 - val_accuracy: 0.9259\n",
            "Epoch 39/100\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2530 - accuracy: 0.9285 - val_loss: 0.2818 - val_accuracy: 0.9249\n",
            "Epoch 40/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2532 - accuracy: 0.9294 - val_loss: 0.2796 - val_accuracy: 0.9233\n",
            "Epoch 41/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2526 - accuracy: 0.9296 - val_loss: 0.2767 - val_accuracy: 0.9226\n",
            "Epoch 42/100\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2525 - accuracy: 0.9292 - val_loss: 0.2798 - val_accuracy: 0.9239\n",
            "Epoch 43/100\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2528 - accuracy: 0.9289 - val_loss: 0.2796 - val_accuracy: 0.9254\n",
            "Epoch 44/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2519 - accuracy: 0.9298 - val_loss: 0.2812 - val_accuracy: 0.9239\n",
            "Epoch 45/100\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2515 - accuracy: 0.9295 - val_loss: 0.2866 - val_accuracy: 0.9217\n",
            "Epoch 46/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2518 - accuracy: 0.9303 - val_loss: 0.2820 - val_accuracy: 0.9218\n",
            "Epoch 47/100\n",
            "1231/1875 [==================>...........] - ETA: 1s - loss: 0.2484 - accuracy: 0.9300"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/min-is/Chapman-University/CPSC 393 - Machine Learning/Classwork/9 - Neural Networks IV.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://github/min-is/Chapman-University/CPSC%20393%20-%20Machine%20Learning/Classwork/9%20-%20Neural%20Networks%20IV.ipynb#ch0000004vscode-vfs?line=14'>15</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m\"\u001b[39m, optimizer\u001b[39m=\u001b[39mkb\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mSGD(\u001b[39m0.01\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell://github/min-is/Chapman-University/CPSC%20393%20-%20Machine%20Learning/Classwork/9%20-%20Neural%20Networks%20IV.ipynb#ch0000004vscode-vfs?line=15'>16</a>\u001b[0m \tmetrics\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://github/min-is/Chapman-University/CPSC%20393%20-%20Machine%20Learning/Classwork/9%20-%20Neural%20Networks%20IV.ipynb#ch0000004vscode-vfs?line=17'>18</a>\u001b[0m \u001b[39m#fit the model (same as SKlearn)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://github/min-is/Chapman-University/CPSC%20393%20-%20Machine%20Learning/Classwork/9%20-%20Neural%20Networks%20IV.ipynb#ch0000004vscode-vfs?line=18'>19</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(trainX,trainY, epochs \u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(testX, testY))\n",
            "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=61'>62</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=62'>63</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///usr/local/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=63'>64</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/keras/engine/training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/keras/engine/training.py?line=1376'>1377</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/keras/engine/training.py?line=1377'>1378</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/keras/engine/training.py?line=1378'>1379</a>\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/keras/engine/training.py?line=1379'>1380</a>\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/keras/engine/training.py?line=1380'>1381</a>\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/keras/engine/training.py?line=1381'>1382</a>\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/keras/engine/training.py?line=1382'>1383</a>\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> <a href='file:///usr/local/lib/python3.8/site-packages/keras/engine/training.py?line=1383'>1384</a>\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/keras/engine/training.py?line=1384'>1385</a>\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/keras/engine/training.py?line=1385'>1386</a>\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
            "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=147'>148</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=148'>149</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=149'>150</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=150'>151</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=151'>152</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=911'>912</a>\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=913'>914</a>\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=914'>915</a>\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=916'>917</a>\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=917'>918</a>\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
            "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=943'>944</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=944'>945</a>\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=945'>946</a>\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=946'>947</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=947'>948</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=948'>949</a>\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=949'>950</a>\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=950'>951</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
            "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=2952'>2953</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=2953'>2954</a>\u001b[0m   (graph_function,\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=2954'>2955</a>\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=2955'>2956</a>\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=2956'>2957</a>\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
            "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1848'>1849</a>\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1849'>1850</a>\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1850'>1851</a>\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1851'>1852</a>\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1852'>1853</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1853'>1854</a>\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1854'>1855</a>\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1855'>1856</a>\u001b[0m     args,\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1856'>1857</a>\u001b[0m     possible_gradient_type,\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1857'>1858</a>\u001b[0m     executing_eagerly)\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1858'>1859</a>\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
            "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=496'>497</a>\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=497'>498</a>\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=498'>499</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=499'>500</a>\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=500'>501</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=501'>502</a>\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=502'>503</a>\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=503'>504</a>\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=504'>505</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=505'>506</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=506'>507</a>\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=507'>508</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=510'>511</a>\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=511'>512</a>\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
            "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py?line=51'>52</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py?line=52'>53</a>\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py?line=53'>54</a>\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py?line=54'>55</a>\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py?line=55'>56</a>\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py?line=56'>57</a>\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import regularizers\n",
        "\n",
        "# build structure of the model\n",
        "model = kb.Sequential([\n",
        "    kb.layers.Dense(250, input_shape =[784]), #input\n",
        "    kb.layers.Dense(100),\n",
        "    kb.layers.Dense(50),\n",
        "    kb.layers.Dense(30),\n",
        "    kb.layers.Dense(20),\n",
        "    kb.layers.Dense(10),\n",
        "    kb.layers.Dense(10, activation = \"softmax\") #output\n",
        "])\n",
        "\n",
        "# compile model\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=kb.optimizers.SGD(0.01),\n",
        "\tmetrics=[\"accuracy\"])\n",
        "\n",
        "#fit the model (same as SKlearn)\n",
        "model.fit(trainX,trainY, epochs = 100, validation_data=(testX, testY))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ8qaSPf3PkM"
      },
      "source": [
        "Using this method of adding a penalty on the weights will use the default **regularization strength**. Remember that regularization strength tells the model how MUCH to penalize large weights. If regularization strength is 0, then there's no penalty. If it's greater than 0, there will be more and more penalization as regularization strength gets bigger. \n",
        "\n",
        "Typically these defaults are pretty good (check them out [here](https://keras.io/api/layers/regularizers/)). BUT if you'd like more control over the regularization strength, you can also pass a `regulizers` object to the `kernel_regularizer` argument. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tArUd7CV3PkN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1875/1875 [==============================] - 5s 2ms/step - loss: 5.9507 - accuracy: 0.8363 - val_loss: 1.6470 - val_accuracy: 0.8621\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x16142c7f0>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tensorflow.keras import regularizers\n",
        "\n",
        "# build structure of the model\n",
        "model = kb.Sequential([\n",
        "    kb.layers.Dense(250, input_shape =[784]), #input\n",
        "    kb.layers.Dense(100, kernel_regularizer = \"l1\"),\n",
        "    kb.layers.Dense(50, kernel_regularizer = \"l2\"),\n",
        "    kb.layers.Dense(30, kernel_regularizer = regularizers.L1L2(l1 = 0.0000001, l2 = 0.0001)),\n",
        "    kb.layers.Dense(20, kernel_regularizer = regularizers.L1(0.01)),\n",
        "    kb.layers.Dense(10, kernel_regularizer = regularizers.L2(0.002)),\n",
        "    kb.layers.Dense(10, activation = \"softmax\") #output\n",
        "])\n",
        "\n",
        "# compile model\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=kb.optimizers.SGD(0.01),\n",
        "\tmetrics=[\"accuracy\"])\n",
        "\n",
        "#fit the model (same as SKlearn)\n",
        "model.fit(trainX,trainY, epochs = 1, validation_data=(testX, testY))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZYTR4t53PkN"
      },
      "source": [
        "## Bagging \n",
        "\n",
        "When we use Bagging, our goal is to have many *slightly different* models that we then average together to make a prediction. Traditionaly (like in Random Forests) we implement this by **bootstrapping** samples of data to train on. Bootstrapping is sampling *with replacement* so that each bootstrapped sample may have some data poitns that occur multiple times, and others that do not occur at all. Thus each model is trained on *slightly* different data.\n",
        "\n",
        "This helps prevent overfitting because while each model might overfit to the noise/quirks in the sample it sees, no two models will overfit to the SAME noise/quirks because they're seeing different data. Thus, what the models have in common are the broad/general patterns that apply to *any* sample of data. \n",
        "\n",
        "It turns out that NNs don't *necessarily* need bagging in order to get several *slightly different* networks. NNs are sensitive to things like:\n",
        "\n",
        "- the initial random weights selected before we start training\n",
        "- which points end up in which mini-batch when training\n",
        "- etc...\n",
        "\n",
        "And thus it may be possible to train several networks without bagging and still end up with an ensemble of slightly different networks (you can still use bagging though, but its not easy with Keras).\n",
        "\n",
        "\n",
        "We won't build these too often, so I won't spend time going over it here, but [here's](https://www.adriangb.com/scikeras/stable/notebooks/Meta_Estimators.html#4.-Bagging-ensemble) an example.\n",
        "\n",
        "## Dropout\n",
        "\n",
        "Dropout is one of the most POPULAR methods of regularization in NNs because it is easy to implement, works with pretty much any NN structure, and is very effective at reducing overfitting. Dropout works on the same principles as *Bagging* but is more efficiently implemented.\n",
        "\n",
        "In Dropout we mask--aka set to 0--some of the outputs of a layer before feeding them into the next layer. We do this randomly and it changes each time we take a pass through the network, so as the model is training, it basically never knows which inputs will not be there...so it can't \"put all its eggs in one basket\" so to speak. In other words, the model cannot over-rely on any connections, because sometimes those nodes will just be gone. \n",
        "\n",
        "In keras, [Dropout](https://keras.io/api/layers/regularization_layers/dropout/) is implemented through a `Dropout()` layer, which you can stick between any two layers in your network. When data passes through the Dropout layer, it will randomly set some of the inputs into the `Dropout()` layer to 0 based on the `rate` argument which is a value between 0 and 1 indicating the fraction of nodes in that layer to set to 0/drop. 0 would mean we do not drop any values. 1 means we drop all values. Typically we'll set this value to something like 0.2, 0.3, or 0.5. \n",
        "\n",
        "**Note**: Dropout is only applied during *training*. When making predictions with the model, we do not dropout any nodes. \n",
        "\n",
        " Let's take the MNIST model we built in Classwork 5 and add Dropout to the layers. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XMSTw0n_3PkO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 1.0417 - accuracy: 0.6508 - val_loss: 0.4269 - val_accuracy: 0.8836\n",
            "Epoch 2/3\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.7054 - accuracy: 0.7792 - val_loss: 0.3854 - val_accuracy: 0.8929\n",
            "Epoch 3/3\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.6514 - accuracy: 0.8002 - val_loss: 0.3626 - val_accuracy: 0.8967\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1632e1ee0>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# build structure of the model\n",
        "model = kb.Sequential([\n",
        "    kb.layers.Dense(250, input_shape =[784]),\n",
        "    kb.layers.Dropout(0.5),\n",
        "    kb.layers.Dense(100),\n",
        "    kb.layers.Dropout(0.3),\n",
        "    kb.layers.Dense(50),\n",
        "    kb.layers.Dropout(0.2),\n",
        "    kb.layers.Dense(30),\n",
        "    kb.layers.Dropout(0.3),\n",
        "    kb.layers.Dense(20),\n",
        "    kb.layers.Dropout(0.2),\n",
        "    kb.layers.Dense(10),\n",
        "    kb.layers.Dense(10, activation = \"softmax\") #output\n",
        "])\n",
        "\n",
        "# compile model\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=kb.optimizers.SGD(0.01),\n",
        "\tmetrics=[\"accuracy\"])\n",
        "\n",
        "#fit the model (same as SKlearn)\n",
        "model.fit(trainX,trainY, epochs = 3, validation_data=(testX, testY))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpYFgowi3PkO"
      },
      "source": [
        "## Early Stopping \n",
        "\n",
        "Early Stopping regularizes our model by stopping training once the validation loss no longer improves. Remember that the pattern of overfitting shows that overfitting happens when the training loss continues to improve while the validation loss stops improving or gets worse. \n",
        "\n",
        "In Keras, [Early Stopping](https://keras.io/api/callbacks/early_stopping/) is implemented through a `callback` which is an object that has access to your model information during training and can make changes to the training process during training. \n",
        "\n",
        " Let's take the MNIST model we built in Classwork 5 and add Early Stopping. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BhZSGzcx3PkO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.4988 - accuracy: 0.8511 - val_loss: 0.3266 - val_accuracy: 0.9086\n",
            "Epoch 2/3\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3277 - accuracy: 0.9073 - val_loss: 0.2936 - val_accuracy: 0.9173\n",
            "Epoch 3/3\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3083 - accuracy: 0.9119 - val_loss: 0.2913 - val_accuracy: 0.9167\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x163180c70>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# tells keras to stop training when the validation loss no longer improves for 3 epochs in a row\n",
        "\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'loss', patience = 3)]\n",
        "\n",
        "# build structure of the model\n",
        "model = kb.Sequential([\n",
        "    kb.layers.Dense(250, input_shape =[784]), #input\n",
        "    kb.layers.Dense(100),\n",
        "    kb.layers.Dense(50),\n",
        "    kb.layers.Dense(30),\n",
        "    kb.layers.Dense(20),\n",
        "    kb.layers.Dense(10),\n",
        "    kb.layers.Dense(10, activation = \"softmax\") #output\n",
        "])\n",
        "\n",
        "# compile model\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=kb.optimizers.SGD(0.01),\n",
        "\tmetrics=[\"accuracy\"])\n",
        "\n",
        "#fit the model (same as SKlearn)\n",
        "model.fit(trainX,trainY, epochs = 3, validation_data=(testX, testY), callbacks = callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETUGlNXc3PkP"
      },
      "source": [
        "## Batch Norm\n",
        "\n",
        "Batch Normalization was actually proposed as a way to handle something called \"covariate shift\" but much like many things in Deep Learning, we discovered by accident that it had a regularizing effect on models. We don't really know why it works, but hey if it works, it works. Some propose that the \"noise\" inserted into our model by using the *estimate* of the mean and variance for each batch is what causes this regularization. It also can speed up training and allow us to use larger learning rates because it smooths out the loss surface we're trying to descend. \n",
        "\n",
        "Batch Norm is similar to z-scoring, but we do it *between* layers. This ensures that the input to each layer in our network is stable. \n",
        "\n",
        "Batch Norm first takes the output and centers it based on the estimated mean of the outputs of a layer, scales it based on the estimated variance/standard deviation of the outputs of a layer, and then scaling and shifting it based on learned parameters $\\gamma$ and $\\beta$ which allow the output to have a different mean ($\\beta$) and standard deviation ($\\gamma$) than 0 and 1.\n",
        "\n",
        "Thus we take the outputs $\\mathbf{x}$ and turn them into\n",
        "\n",
        "$$\\beta + \\gamma * \\frac{\\mathbf{x} - \\mu_B}{\\sqrt{(\\sigma_B + \\epsilon)}}$$\n",
        "\n",
        "Where $\\beta$ is the new mean of the outputs and $\\gamma$ is the new standard deviation of the outputs. $\\epsilon$ is just a tiny value that helps prevent us from dividing by 0. \n",
        "\n",
        "$\\gamma$ and $\\beta$ are learned during training through backpropagation/gradient descent just like the weights and biases in our model. \n",
        "\n",
        "In Keras, we can use a [`BatchNormalization()`](https://keras.io/api/layers/normalization_layers/batch_normalization/) layer in order to implement Batch Normalization.  Let's take the MNIST model we built in Classwork 5 and add Batch Normalization to the layers. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "T7zxc8HK3PkP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.5909 - accuracy: 0.8307 - val_loss: 0.3391 - val_accuracy: 0.9039\n",
            "Epoch 2/3\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3866 - accuracy: 0.8877 - val_loss: 0.3040 - val_accuracy: 0.9129\n",
            "Epoch 3/3\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3544 - accuracy: 0.8966 - val_loss: 0.2960 - val_accuracy: 0.9128\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x162ebf820>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# build structure of the model\n",
        "model = kb.Sequential([\n",
        "    kb.layers.Dense(250, input_shape =[784]), #input\n",
        "    kb.layers.BatchNormalization(),\n",
        "    kb.layers.Dense(100),\n",
        "    kb.layers.BatchNormalization(),\n",
        "    kb.layers.Dense(50),\n",
        "    kb.layers.BatchNormalization(),\n",
        "    kb.layers.Dense(30),\n",
        "    kb.layers.BatchNormalization(),\n",
        "    kb.layers.Dense(20),\n",
        "    kb.layers.BatchNormalization(),\n",
        "    kb.layers.Dense(10),\n",
        "    kb.layers.Dense(10, activation = \"softmax\") #output\n",
        "])\n",
        "\n",
        "# compile model\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=kb.optimizers.SGD(0.01),\n",
        "\tmetrics=[\"accuracy\"])\n",
        "\n",
        "#fit the model (same as SKlearn)\n",
        "model.fit(trainX,trainY, epochs = 3, validation_data=(testX, testY))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m6d06wD3PkP"
      },
      "source": [
        "# On Your Own\n",
        "\n",
        "We can use multiple methods of regularization at once (although there are some rules of thumb, like use penalties when your network is small, and Dropout when it is large...). \n",
        "\n",
        "Take the MNIST NN and add multiple methods of regularization as you see fit. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "gES2MxqA3PkQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "1875/1875 [==============================] - 41s 21ms/step - loss: 0.4998 - accuracy: 0.8543 - val_loss: 0.3180 - val_accuracy: 0.9101\n",
            "Epoch 2/3\n",
            " 583/1875 [========>.....................] - ETA: 26s - loss: 0.3665 - accuracy: 0.8947"
          ]
        }
      ],
      "source": [
        "# build structure of the model\n",
        "model = kb.Sequential([\n",
        "    kb.layers.Dense(1000, input_shape =[784]),\n",
        "    kb.layers.BatchNormalization(),\n",
        "    kb.layers.Dense(950),\n",
        "    kb.layers.Dense(800),\n",
        "    kb.layers.BatchNormalization(),\n",
        "    kb.layers.Dense(550),\n",
        "    kb.layers.Dense(300),\n",
        "    kb.layers.BatchNormalization(),\n",
        "    kb.layers.Dense(100),\n",
        "    kb.layers.Dense(75),\n",
        "    kb.layers.BatchNormalization(),\n",
        "    kb.layers.Dense(50),\n",
        "    kb.layers.Dense(30),\n",
        "    kb.layers.BatchNormalization(),\n",
        "    kb.layers.Dense(20),\n",
        "    kb.layers.Dense(10),\n",
        "    kb.layers.Dense(10, activation = \"softmax\") #output\n",
        "])\n",
        "\n",
        "# compile model\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=kb.optimizers.SGD(0.01),\n",
        "\tmetrics=[\"accuracy\"])\n",
        "\n",
        "#fit the model (same as SKlearn)\n",
        "model.fit(trainX,trainY, epochs = 3, validation_data=(testX, testY), callbacks = callbacks)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
