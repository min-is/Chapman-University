{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UNG6sq7OH0d"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from plotnine import *\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# make sure you have these to make dendrograms!-------\n",
        "import scipy.cluster.hierarchy as sch\n",
        "from matplotlib import pyplot as plt\n",
        "#-----------------------------------------------------\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AWAaeU0OH0e"
      },
      "source": [
        "# 0. Together\n",
        "\n",
        "Hierarchical clustering (as the name suggests) assumes that the clusters in the data have a hierarchical relationship. For example, in a McDonald's food dataset we could have clusters like: Dessert, Drinks, Sandwhiches, Other. Within Sandwhiches we could have chicken sandwhiches, Burgers, Vegan Sandwhiches...within Burgers we could have smaller, lower calorie options, and bigger, more substantial burgers...etc.\n",
        "\n",
        "Blood cells is another great example of a hierarchical relationship: ![blood hierarchy](https://community.jmp.com/t5/image/serverpage/image-id/16820i93FA5BD273E0A842/image-dimensions/340x314?v=1.0)\n",
        "\n",
        "Hierarchical *Agglomeretive* Clustering (which we perform here), goes bottom up: every datapoint starts as it's own singleton cluster, and at each step, we merge the two closest clusters together until all data points are in one big cluster. In order to decide which clusters are closest, we need to choose two things:\n",
        "\n",
        "* **distance metric**: this is a measure that helps us define how close together two *data points* are. Euclidean distance is a common distance metric that you may be familiar with, but there is also cosine distance, manhattan distance, hamming distance, and even custom distance functions (like [levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance) between two strings!)\n",
        "* **linkage criteria**: this is a measure of how close two *clusters* are. Because (most) clusters have more than one point, we need to define what it means for two clusters to be close.\n",
        "    * **Single Linkage**: the distance between two clusters (A and B) as the minimum distance between any point in A and any point in B ![single linkage](https://community.jmp.com/t5/image/serverpage/image-id/16823iF32133201794C0A4/image-dimensions/251x242?v=1.0)\n",
        "    * **Average Linkage**: the distance between two clusters (A and B) as the average distance between points in A and points in B. ![average linkage](https://community.jmp.com/t5/image/serverpage/image-id/16824iDD065DCADD44D5EC/image-dimensions/275x307?v=1.0)\n",
        "    * **Complete Linkage**: the distance between two clusters (A and B) as the maximum distance between any point in A and any point in B. ![complete linkage](https://community.jmp.com/t5/image/serverpage/image-id/16825i39A778742E501081/image-dimensions/277x245?v=1.0)\n",
        "    * **Centroid Method**: the distance between two clusters (A and B) as the distance between their respective mean vectors (centroids). ![centroid method](https://community.jmp.com/t5/image/serverpage/image-id/16826iFC5E179AEFFF1252/image-dimensions/260x268?v=1.0)\n",
        "    * **Ward's Method** (default): the distance between two clusters (A and B) as the Sum of Squared Errors when combining the two clusters together. ![ward's method](https://community.jmp.com/t5/image/serverpage/image-id/16827iA35DD99890489DB2/image-dimensions/253x164?v=1.0)\n",
        "    * and MORE! You could technically define this anyway you wanted.\n",
        "\n",
        "\n",
        "## 0.1 Dendrograms\n",
        "\n",
        "### 0.1.1 Diffuse Overlapping Clusters\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1OCGvoe2FtZdIm0NnXbuwDo49E9CDrkIc\" width = 600px />\n",
        "\n",
        "### 0.1.2 Highly Separable Clusters\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1xpYaV-Pa1agH7H-OLzRvCcSWU-k78Uzf\" width = 600px />\n",
        "\n",
        "### 0.1.3 Let's Try on Our Own\n",
        "\n",
        "#### 1.\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1ilZW8x11EjSAYub7jFM_kN4DxLMsgvOx\" width = 500px />\n",
        "\n",
        "#### 2.\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1bz5MM_HZe30uLLesgCXZkEhfcgmVHG43\" width = 500px />\n",
        "\n",
        "#### 3.\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1b3VHTE0WqmgtVa8ywtJX6sQkvjVadQ6D\" width = 500px />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzoy9FZ5OH0g"
      },
      "source": [
        "# 1. Linkage Critera\n",
        "\n",
        "Let's build a few functions that return the distance between two clusters. Each of these functions take in two dataframes, `A` and `B` that contain the datapoints for the two respective clusters (*number of features can vary*).\n",
        "\n",
        "The functions take in two arguments:\n",
        "\n",
        "* `A`: an N1 x P dataframe containing the data points in cluster A. (N1 is the number of data points in cluster A; P is the number of features used)\n",
        "* `B`: an N2 x P dataframe containing the data points in cluster B. (N2 is the number of data points in cluster B; P is the number of features used)\n",
        "\n",
        "\n",
        "The function should calculate and return the distance between the clusters (assume you're using euclidean distance for all of these) according to their respective linkage criterion (single, average, and complete). Remember a) that you need to calculate the distance between *every* point in `A` and *every* point in `B` and b) [`np.linalg.norm()`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html).\n",
        "\n",
        "### *Question*\n",
        "\n",
        "1. To calculate the distance between two clusters `A` (N1 x P) and `B` (N2 x P), how many distances (between 2 data points) would you have to calculate?\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" width = 200px />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21HfsA8COH0h"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n",
        "def single(A,B):\n",
        "    pass\n",
        "\n",
        "\n",
        "def average(A,B):\n",
        "    pass\n",
        "\n",
        "\n",
        "def complete(A,B):\n",
        "    pass\n",
        "\n",
        "\n",
        "### /YOUR CODE HERE ###\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcvsPMegOH0i"
      },
      "outputs": [],
      "source": [
        "# check if your functions are working\n",
        "\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/HAC1.csv\")\n",
        "\n",
        "dA = df.loc[df.cluster == \"A\"] # cluster A\n",
        "dB = df.loc[df.cluster == \"B\"] # cluster B\n",
        "dC = df.loc[df.cluster == \"C\"] # cluster C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myb6d9sqOH0i"
      },
      "outputs": [],
      "source": [
        "# if complete() is correct, this will print true\n",
        "completePASS = abs(complete(dA[[\"x\",\"y\"]], dB[[\"x\",\"y\"]]) - 4.718047025872837) <= 0.01\n",
        "print(\"Complete:\", completePASS)\n",
        "\n",
        "# if average() is correct, this will print true\n",
        "averagePASS = abs(average(dA[[\"x\",\"y\"]], dB[[\"x\",\"y\"]]) - 2.734811240314461) <= 0.01\n",
        "print(\"Average:\", averagePASS)\n",
        "\n",
        "# if single() is correct, this will print true\n",
        "singlePASS = abs(single(dA[[\"x\",\"y\"]], dB[[\"x\",\"y\"]]) - 0.7361237342164363) <= 0.01\n",
        "print(\"Single:\", singlePASS)\n",
        "              "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6eMK8C8OH0i"
      },
      "source": [
        "Using the dataset `df` below, \n",
        "\n",
        "1. plot the clusters using ggplot, color by cluster\n",
        "2. use your functions `single()`, `average()`, and `complete()` to calculate the distances between each pair of clusters. \n",
        "\n",
        "### *Question*\n",
        "\n",
        "3. Look at which clusters are considered \"close\" and \"far\" in different methods. Are there differences between which are furthest/closest between methods? What are they? \n",
        "\n",
        "4. Describe why you think you see these differences.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" width = 200px />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "if0WHj8VOH0j"
      },
      "outputs": [],
      "source": [
        "# plot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cu9lmtJMOH0k"
      },
      "outputs": [],
      "source": [
        "# calculate distances (this will likely take a few min to run)\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "#---single------\n",
        "s_AB = ###\n",
        "s_AC = ###\n",
        "s_BC = ###\n",
        "\n",
        "print(\"AB:\", s_AB)\n",
        "print(\"AC:\", s_AC)\n",
        "print(\"BC:\", s_BC)\n",
        "print(\"\\n\")\n",
        "#---average-----\n",
        "a_AB = ###\n",
        "a_AC = ###\n",
        "a_BC = ###\n",
        "\n",
        "print(\"AB:\", a_AB)\n",
        "print(\"AC:\", a_AC)\n",
        "print(\"BC:\", a_BC)\n",
        "print(\"\\n\")\n",
        "#---complete----\n",
        "c_AB = ###\n",
        "c_AC = ###\n",
        "c_BC = ###\n",
        "\n",
        "print(\"AB:\", c_AB)\n",
        "print(\"AC:\", c_AC)\n",
        "print(\"BC:\", c_BC)\n",
        "print(\"\\n\")\n",
        "\n",
        "### /YOUR CODE HERE ###\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HWBq7w2OH0k"
      },
      "source": [
        "# 2. Exploring Linkage Critera\n",
        "\n",
        "Using the predictors listed in `predictors`, perform HAC on the [burger king dataset](\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/burger-king-items.txt) using sklearn and the following linkage critera:\n",
        "\n",
        "* single linkage\n",
        "* average linkage\n",
        "* complete linkage\n",
        "* ward's method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L5Iqx2wOH0l"
      },
      "source": [
        "### *Question*\n",
        "1. Plot and compare the different clusters/dendrograms that you get. What do you notice is similar/different?\n",
        "\n",
        "2. Think hard: what do the dendrograms tell you about the data?\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" width = 200px />\n",
        "\n",
        "(NOTE: see [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering) if you need a refresher on how to set linkage criteria in sklearn, and [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html) for how to set it for the dendrogram)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbR_lnlFOH0l"
      },
      "outputs": [],
      "source": [
        "burg = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/burger-king-items.txt\",\n",
        "                  sep = \"\\t\")\n",
        "\n",
        "predictors = [\"Calories\", \"Protein(g)\", \"Fat(g)\", \"Sodium(mg)\", \"Carbs(g)\", \"Sugar(g)\"]\n",
        "z = StandardScaler()\n",
        "burg[predictors] = z.fit_transform(burg[predictors])\n",
        "\n",
        "burg.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Alc0Qhp5OH0l"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n",
        "### \\YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euxjww62OH0m"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n",
        "### \\YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GPkQndLOH0m"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n",
        "### \\YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MH6bp5FNOH0m"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n",
        "### \\YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBbjDQMeOH0m"
      },
      "source": [
        "# 3. How Linkage Affects Clustering\n",
        "\n",
        "### **Questions**\n",
        "\n",
        "1. Generally, how could linkage critera affect the type/shape of clusters that HAC creates?\n",
        "\n",
        "2. Now that you've built your linkage criteria functions, and have seen how different linkage criteria affect how *close* the HAC algorithm thinks two clusters are, which linkage criteria would especially encourage *cohesive* clusters (remember that density and cohesion are different, cohesion is when all data points are relatively near *all* the data points in it's cluster, density is when data points are relatively near their *closest neighbors* in the cluster). "
      ]
    }
  ],
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "HierarchicalAgglomeretive_Class19.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}