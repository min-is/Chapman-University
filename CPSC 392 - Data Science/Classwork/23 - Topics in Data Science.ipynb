{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "code_folding": [
          0
        ],
        "id": "lZwO4mHBVLgk"
      },
      "outputs": [],
      "source": [
        "# import the necessary packages\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from plotnine import *\n",
        "\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier # Decision Tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn import metrics \n",
        "from sklearn.preprocessing import StandardScaler #Z-score variables\n",
        "\n",
        "from sklearn.model_selection import train_test_split # simple TT split cv\n",
        "from sklearn.model_selection import KFold # k-fold cv\n",
        "from sklearn.model_selection import LeaveOneOut #LOO cv\n",
        "from sklearn.model_selection import cross_val_score # cross validation metrics\n",
        "from sklearn.model_selection import cross_val_predict # cross validation metrics\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.metrics import plot_confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-bJqQkIVLgl"
      },
      "source": [
        "# 1. The Ensemble\n",
        "A common theme in applied Machine Learning is *The Ensemble Method*. Ensemble methods use multiple machine learning models (these models can be the same type or different algorithms entirely). The idea is that using ensembles improves predictive performance, because even though our models are sometimes incorrect, it's unlikely that a MAJORITY of the models in our ensemble will all be incorrect in the exact same way each time. Therefore in aggregate, we will get a more accurate model.\n",
        "\n",
        "Each model gets a \"vote\" about what category a data point should be in (ensemble methods also work for continuous outcomes, but here we'll focus on categorical ones). Whichever category gets the most \"votes\" is the category we choose for that data point. \n",
        "\n",
        "To combat overfitting and reduce potential *over-reliance* on a small number of features, we can use the two following techniques when creating models for our ensemble:\n",
        "\n",
        "* **Bagging (Bootstrap Aggregating)**: Instead of using all of our training data to train each model in our sample we use **bootstrapping** to choose the samples we will include.\n",
        "    * **Bootstrapping** is when you randomly sample data points *with replacement*, meaning that a data point can be included in your bootstrapped sample *more* than once, OR not at all.\n",
        "    \n",
        "\n",
        " ## Bootstrapping (before we start)\n",
        " The code block below gives an example of how we can use `np.random.choice()` to bootstrap samples from a dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcVE0EXjVLgn",
        "outputId": "3a8b9dec-6010-4193-a645-b2f070cc7192"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Jasper</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Luke</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Susan</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>Lisa</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Kayne</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Lydia</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Jasper</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>Lisa</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>John</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Anthony</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Jane</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Leia</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Alex</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Peter</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Greg</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Bart</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>John</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>Marie</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Greg</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>Marie</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>Torrence</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>Lisa</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>Torrence</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Leia</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Jane</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Rhett</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Georgia</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Erik</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Mark</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Peter</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Daniel</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Mark</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        name  age\n",
              "14    Jasper   26\n",
              "19      Luke   19\n",
              "7      Susan   24\n",
              "28      Lisa   20\n",
              "10     Kayne   25\n",
              "11     Lydia   17\n",
              "14    Jasper   26\n",
              "28      Lisa   20\n",
              "17      John   17\n",
              "23   Anthony   17\n",
              "13      Jane   17\n",
              "20      Leia   23\n",
              "0       Alex   20\n",
              "12     Peter   22\n",
              "5       Greg   26\n",
              "29      Bart   18\n",
              "17      John   17\n",
              "31     Marie   18\n",
              "5       Greg   26\n",
              "31     Marie   18\n",
              "25  Torrence   17\n",
              "28      Lisa   20\n",
              "25  Torrence   17\n",
              "20      Leia   23\n",
              "13      Jane   17\n",
              "16     Rhett   19\n",
              "9    Georgia   23\n",
              "8       Erik   26\n",
              "24      Mark   26\n",
              "12     Peter   22\n",
              "6     Daniel   18\n",
              "24      Mark   26"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# How to BOOTSTRAP\n",
        "\n",
        "## simple bootstrapping example of names dataframe \n",
        "np.random.seed(1234)\n",
        "\n",
        "names = [\"Alex\", \"Charlie\", \"Addison\", \"James\", \"Blake\", \"Greg\", \"Daniel\", \"Susan\", \"Erik\", \"Georgia\", \"Kayne\",\n",
        "         \"Lydia\", \"Peter\", \"Jane\", \"Jasper\", \"Link\", \"Rhett\", \"John\", \"Miranda\", \"Luke\", \"Leia\", \"Janet\", \"Jung\",\n",
        "         \"Anthony\", \"Mark\", \"Torrence\", \"Bonnie\", \"Rudy\", \"Lisa\", \"Bart\", \"Tina\", \"Marie\"]\n",
        "\n",
        "# create data\n",
        "names_df = pd.DataFrame({\"name\": names, \"age\": np.random.randint(17,27, len(names))})\n",
        "names_df\n",
        "\n",
        "# bootstrap a sample\n",
        "# np.random.choice(possible values, number of samples to choose, sample with replacement?)\n",
        "names_index = np.random.choice(range(0,names_df.shape[0]), # possible indices\n",
        "                               names_df.shape[0], # how many to choose\n",
        "                               replace = True) # with replacement\n",
        "names_boot = names_df.iloc[names_index]\n",
        "\n",
        "# notice how Lisa shows up more than once?\n",
        "names_boot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVqhra-0VLgo"
      },
      "source": [
        "* **Random Feature Selection**: Instead of using all the available features/predictors in our dataset for every model, for each model we randomly choose a different subset of features to use when training. This helps our ensemble generalize, because it doesn't become overly reliant on one feature (since that feature might not appear in every model).\n",
        "\n",
        "While ensemble methods take a lot of computational power (you're training MANY models instead of just one), in practice they're often really useful. An incredibly popular ensemble method is the **Random Forest** which is an ensemble method that uses a bunch of decision trees along with Bagging and Random Feature selection to generate the ensemble.\n",
        "\n",
        "## 1.1 Building a Random Forest\n",
        "\n",
        "Let's build a tiny random forest function of our own! The `Forest()` that takes in 5 arguments:\n",
        "\n",
        "* `n_samples` (**integer**): number of bootstrapped samples to use to train each decision tree.\n",
        "* `n_features` (**integer**): number of randomly selected features from your data set to use when training.\n",
        "* `n_trees` (**integer**): how many decision trees to create for the ensemble.\n",
        "* `X` (**data frame**): the *already* z-scored predictor data to be used.\n",
        "* `y` (**data frame**): the outcome data to be used (`X` and `y` are the same length, and the $i^{th}$ element of `X` corresponds to the $i^{th}$ element of `y`)\n",
        "\n",
        "The function should:\n",
        "\n",
        "1. (DONE FOR YOU) use a for loop to create `n_trees` models and store them in a list called `forest` (yes! You can store fitted decision trees in a list!)\n",
        "2. (DONE FOR YOU) For each model you should choose use bootstrapping to sample `n_samples` data points to train each model. Remember that boostrapping means sampling WITH replacement (hint: try using [`np.random.choice`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html) to randomly select (*with replacement*) which row numbers/indices to use.\n",
        "3. (DONE FOR YOU) For each model, randomly select `n_features` to use to train your model. (hint: try using [`np.random.choice`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html) to randomly select (*withOUT replacement*) which predictor indices to use.\n",
        "4. For each model, train the model (no need to use model validation, and assume X is already z-scored).\n",
        "5. (DONE FOR YOU) Return a list (`forest`) of dictonaries that look like this (where `tree` is your trained model and `samples_index` is an array of indices for the features/predictors you selected):\n",
        " ```{\"tree\": tree, \"feats\": samples_index}```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UShwkvlsVLgo"
      },
      "outputs": [],
      "source": [
        "def Forest(X, y, n_samples = 1000, n_features = 5, n_trees = 100):\n",
        "    forest = []\n",
        "    \n",
        "    # 1. create models for the ensemble\n",
        "    for i in range(0,n_trees):\n",
        "        \n",
        "        # 2. randomly bootstrap datapoints\n",
        "        possible_rows_index = range(0,X.shape[0])\n",
        "        samples_index = np.random.choice(possible_rows_index, n_samples, replace = True)\n",
        "        \n",
        "        \n",
        "        # 3. randomly choose features\n",
        "        possible_features_index = range(0,X.shape[1])\n",
        "        \n",
        "        if n_features >= X.shape[1]: #if they ask for more features than you have, select all features...\n",
        "            features_index = possible_features_index\n",
        "        else:\n",
        "            features_index = np.random.choice(possible_features_index, n_features, replace = False)\n",
        "        \n",
        "        # 4. select only the rows and features that were randomly selected above and train tree\n",
        "        \n",
        "        \n",
        "        ### YOUR CODE HERE ##################################################################\n",
        "        \n",
        "        # use samples_index to choose only the subset of bootstrapped rows from X and from y\n",
        "        \n",
        "        \n",
        "        # use features_index to choose only the subset of features/predictors from X\n",
        "        \n",
        "        \n",
        "        # create a DecisionTree model\n",
        "        tree = ??? \n",
        "        \n",
        "        # fit the tree using the *subsetted* data you created just now\n",
        "\n",
        "\n",
        "        \n",
        "         ### /YOUR CODE HERE ##################################################################\n",
        "            \n",
        "            \n",
        "        # 5. add tree to forest\n",
        "        forest.append({\"tree\": tree, \"feats\": features_index})\n",
        "    return(forest)       "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZasiDMSPVLgp"
      },
      "source": [
        "## 1.2 Use `Forest()`\n",
        "Using `X1` and `y1` as your training set, call `Forest()` to build an ensemble model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZ4-1MATVLgp"
      },
      "outputs": [],
      "source": [
        "# load in the data\n",
        "X1 = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/X_cols_df.csv\")\n",
        "y1 = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/y_df.csv\")\n",
        "\n",
        "# look at X\n",
        "X1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpQPJDKrVLgp"
      },
      "outputs": [],
      "source": [
        "# look at y\n",
        "y1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeQhmV7ZVLgp"
      },
      "source": [
        "The cell below uses the Forest function that you helped create above to build a random forest using `X1` and `y1`. The `n_features` argument shows that we will randomly select 5 features per tree to use. Our forest will contain 100 trees (as shown by the `n_trees` agument)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eIpewz5VLgq"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ##################################################################\n",
        "\n",
        "my_forest = Forest(X1,\n",
        "                   y1,\n",
        "                   n_features = 5,\n",
        "                   n_trees = 100) ### call Forest and create an ensemble with 100 trees and 5 features per tree\n",
        "\n",
        "### /YOUR CODE HERE ##################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5_tTzjjVLgq"
      },
      "source": [
        "## 1.3 Comparing Ensemble to an Individual Model\n",
        "\n",
        "- Use the `ForestPredictor()` function below (which takes in the ensemble created by `Forest()` and data) to generate predictions (basically like a `.predict()` function for our custom `Forest()` model) for `X2`, our *test* set.\n",
        "- Use the `ForestPredictor()` function below (which takes in the ensemble created by `Forest()` and data) to generate predictions for `X1`, our *train* set.\n",
        "- calculate the accuracy of your ensemble.\n",
        "- calculate the accuracy for ONE of your ensemble models by using `oneModel = my_forest[0]` to grab the first model of your ensemble. \n",
        "\n",
        "### 1.3.1\n",
        "In this example, does an ensemble method do *better* (in terms of train accuracy) than an individual decision tree? Explain how you figured this out.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" width = 200px />\n",
        "\n",
        "\n",
        "## `ForestPredictor()` function\n",
        "The code below is fully functional, no need to change anything. It takes in two arguments:\n",
        "\n",
        "- `forest` which should be a trained Random forest made by the `Forest()` function.\n",
        "- `X` which is the data you want to use to predict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbPgbLAyVLgq"
      },
      "outputs": [],
      "source": [
        "# NO NEED TO CHANGE ANYTHING HERE\n",
        "def ForestPredictor(forest, X):\n",
        "    import operator\n",
        "    from collections import Counter\n",
        "\n",
        "    ps = []\n",
        "\n",
        "    # get predictions from each model\n",
        "    for model in forest:\n",
        "        tree = model[\"tree\"]\n",
        "        X_sub = X.iloc[:, model[\"feats\"]]\n",
        "\n",
        "        p = tree.predict(X_sub)\n",
        "        ps.append(p)\n",
        "\n",
        "    ps = pd.DataFrame(ps)\n",
        "    \n",
        "    # get ensemble prediction for each data point\n",
        "    predictions = []\n",
        "    \n",
        "    for column_ind in range(0, ps.shape[1]):\n",
        "        ensemble_predict = ps.iloc[:,column_ind]\n",
        "        predictions.append(ensemble_predict.mode()[0])\n",
        "\n",
        "    return(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRrFGX7RVLgq"
      },
      "source": [
        "Import our \"test set\", `X2` and `y2`, and use `ForestPredictor()` to get predictions for `X2` using the Random Forest we built above, `my_forest`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXuLCpqQVLgr"
      },
      "outputs": [],
      "source": [
        "# Import our Test Set\n",
        "X2 = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/X_cols_df2.csv\")\n",
        "y2 = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/y_df2.csv\")\n",
        "\n",
        "# ForestPredict() will take your ensemble and use it to find the predicted values for the test set\n",
        "ensemble_predictions =  ForestPredictor(my_forest, X2) #Call ForestPredictor using my_forest and X_cols_df2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48HhveduVLgr"
      },
      "source": [
        "Now we will calculate the accuracy of our Random Forest as a whole and compare it to a randomly selected tree FROM our forest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVezq74CVLgr"
      },
      "outputs": [],
      "source": [
        "# calculate the accuracy for the ensemble\n",
        "acc_e = accuracy_score(y2, ensemble_predictions)\n",
        "print(\"Test Acc (Ensemble) is: \", acc_e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GW5jBNFVLgr"
      },
      "outputs": [],
      "source": [
        "# calculate the accuracy for the first model\n",
        "oneModel = my_forest[0]\n",
        "\n",
        "acc_o = accuracy_score(y2, oneModel[\"tree\"].predict(X2.iloc[:,oneModel[\"feats\"]]))\n",
        "print(\"Test Acc (OneModel) is: \", acc_o)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sdG_hC1VLgr"
      },
      "outputs": [],
      "source": [
        "ensemble_predictions =  ForestPredictor(my_forest, X1) ### Call ForestPredictor using my_forest and X_cols_df2\n",
        "print(\"Train Acc (Ensemble) is: \", accuracy_score(y1, ensemble_predictions))\n",
        "print(\"Train Acc (OneModel) is: \", accuracy_score(y1, oneModel[\"tree\"].predict(X1.iloc[:,oneModel[\"feats\"]])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMeY2IpXVLgs"
      },
      "source": [
        "## 1.4 Comparing Ensemble to an Individual Models\n",
        "\n",
        "- put the accuracy from your ENSEMBLE model in the code below\n",
        "- run the cell to see a histogram of the individual tree accuracies, and the (dashed line) ensemble accuracy.\n",
        "\n",
        "### 1.4.1\n",
        "Write down your thoughts about this graph. What patterns do you see between individual tree accuracies and ensemble accuracies?\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" width = 200px />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kf8gfvNzVLgs"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ##################################################################\n",
        "ensemble_acc = ???### put your ensemble accuracy here!\n",
        "### /YOUR CODE HERE ##################################################################\n",
        "\n",
        "# calculates the accuracy of each model in the ensemble\n",
        "allAcc = [accuracy_score(y2,my_forest[mod][\"tree\"].predict(X2.iloc[:,my_forest[mod][\"feats\"]])) for mod in range(0,len(my_forest))]\n",
        "\n",
        "# plot individual model accuracies (blue hist) and ensemble accuracy (black line)\n",
        "df = pd.DataFrame({\"acc\": allAcc, \"no\": range(0,len(my_forest))})\n",
        "(ggplot(df, aes(x = \"acc\")) +\n",
        " geom_histogram(color = \"black\", fill = \"lightblue\", binwidth = 0.025) +\n",
        " xlim([0,1]) + theme_minimal() + geom_vline(xintercept = ensemble_acc, linetype = \"dashed\", size = 3) +\n",
        "labs(title = \"Ensemble Accuracy vs. Individual Model Accuracy\",\n",
        "    x = \"Accuracy\",\n",
        "    y = \"Count\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCFQM3HiVLgs"
      },
      "source": [
        "### 1.4.2\n",
        "How does the difference between individual tree accuracies and ensemble accuracies change when you change the number of predictors used in each tree?\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" width = 200px />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeGoLAewVLgs"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ##################################################################\n",
        "n_feat = 249\n",
        "### /YOUR CODE HERE ##################################################################\n",
        "\n",
        "\n",
        "# build a new model with n_feat features\n",
        "my_forest2 = Forest(X1, y1, n_features = n_feat)\n",
        "ensemble_acc2 = accuracy_score(y2, ForestPredictor(my_forest2, X2))\n",
        "\n",
        "\n",
        "# calculates the accuracy of each model in the ensemble\n",
        "allAcc2 = [accuracy_score(y2,my_forest2[mod][\"tree\"].predict(X2.iloc[:,my_forest2[mod][\"feats\"]])) for mod in range(0,len(my_forest2))]\n",
        "\n",
        "\n",
        "# plot individual model accuracies (blue hist) and ensemble accuracy (black line)\n",
        "df = pd.DataFrame({\"acc\": allAcc2, \"no\": range(0,len(my_forest2))})\n",
        "(ggplot(df, aes(x = \"acc\")) +\n",
        " geom_histogram(color = \"black\", fill = \"lightblue\", binwidth = 0.025) +\n",
        " xlim([0,1]) + theme_minimal() + geom_vline(xintercept = ensemble_acc2, linetype = \"dashed\", size = 3) +\n",
        "labs(title = \"Ensemble Accuracy vs. Individual Model Accuracy\",\n",
        "    x = \"Accuracy\",\n",
        "    y = \"Count\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhPAseK-VLgt"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ##################################################################\n",
        "n_feat = 2\n",
        "### /YOUR CODE HERE ##################################################################\n",
        "\n",
        "# build a new model with n_feat features\n",
        "my_forest2 = Forest(X1, y1, n_features = n_feat)\n",
        "ensemble_acc2 = accuracy_score(y2, ForestPredictor(my_forest2, X2))\n",
        "\n",
        "\n",
        "# calculates the accuracy of each model in the ensemble\n",
        "allAcc2 = [accuracy_score(y2,my_forest2[mod][\"tree\"].predict(X2.iloc[:,my_forest2[mod][\"feats\"]])) for mod in range(0,len(my_forest2))]\n",
        "\n",
        "\n",
        "# plot individual model accuracies (blue hist) and ensemble accuracy (black line)\n",
        "df = pd.DataFrame({\"acc\": allAcc2, \"no\": range(0,len(my_forest2))})\n",
        "(ggplot(df, aes(x = \"acc\")) +\n",
        " geom_histogram(color = \"black\", fill = \"lightblue\", binwidth = 0.025) +\n",
        " xlim([0,1]) + theme_minimal() + geom_vline(xintercept = ensemble_acc2, linetype = \"dashed\", size = 3) +\n",
        "labs(title = \"Ensemble Accuracy vs. Individual Model Accuracy\",\n",
        "    x = \"Accuracy\",\n",
        "    y = \"Count\"))\n"
      ]
    }
  ],
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "TopicsInDataScience_Class25.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}